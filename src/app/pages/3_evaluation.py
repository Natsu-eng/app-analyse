import os
import pickle
import numpy as np
import streamlit as st
import pandas as pd
import time
import json
import plotly.express as px
import plotly.graph_objects as go
from src.evaluation.model_plots import ModelEvaluationVisualizer
from src.evaluation.metrics import get_system_metrics, EvaluationMetrics
from utils.report_generator import generate_pdf_report
from src.config.constants import TRAINING_CONSTANTS
from logging import getLogger

logger = getLogger(__name__)

# Import MLflow
try:
    import mlflow
    from mlflow.tracking import MlflowClient
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    mlflow = None
    MlflowClient = None

# Configuration PyArrow
os.environ["PANDAS_USE_PYARROW"] = "0"
try:
    pd.options.mode.dtype_backend = "numpy_nullable"
except Exception:
    pass

# Configuration page
st.set_page_config(page_title="√âvaluation des Mod√®les", page_icon="üìà", layout="wide", initial_sidebar_state="expanded")

# CSS personnalis√©
st.markdown("""
<style>
    .main-header {
        font-size: 2rem;
        color: #2c3e50;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: 600;
        border-bottom: 2px solid #3498db;
        padding-bottom: 1rem;
    }
    .metric-card {
        background: #ffffff;
        border: 1px solid #e1e8ed;
        border-radius: 8px;
        padding: 1rem;
        margin: 0.5rem 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.06);
    }
    .best-model-card {
        border-left: 4px solid #27ae60;
        background: linear-gradient(135deg, #f8fff9 0%, #e8f5e8 100%);
    }
    .metric-title {
        font-size: 0.8rem;
        color: #7f8c8d;
        font-weight: 600;
        text-transform: uppercase;
    }
    .metric-value {
        font-size: 1.6rem;
        font-weight: 700;
        color: #2c3e50;
    }
    .metric-subtitle {
        font-size: 0.7rem;
        color: #95a5a6;
    }
    .performance-high { color: #27ae60; background: #d5f4e6; padding: 2px 6px; border-radius: 4px; }
    .performance-medium { color: #f39c12; background: #fef9e7; padding: 2px 6px; border-radius: 4px; }
    .performance-low { color: #e74c3c; background: #fadbd8; padding: 2px 6px; border-radius: 4px; }
    .tab-content {
        padding: 1rem;
        background: #ffffff;
        border-radius: 8px;
        border: 1px solid #ecf0f1;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_data(ttl=3600, max_entries=5)
def cached_plot(fig, _size_key: str):
    """Cache les figures Plotly avec limite de taille"""
    if fig and hasattr(fig, 'to_json'):
        size_mb = len(json.dumps(fig.to_json())) / (1024**2)
        if size_mb > 10:
            st.warning("‚ö†Ô∏è Graphique volumineux, affichage simplifi√©")
            return None
    return fig

def display_metrics_header(validation):
    """Affiche l'en-t√™te avec m√©triques principales"""
    successful_count = len(validation["successful_models"])
    total_count = validation["results_count"]
    
    st.markdown('<div class="main-header">üìà √âvaluation des Mod√®les</div>', unsafe_allow_html=True)
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        success_rate = (successful_count / total_count * 100) if total_count > 0 else 0
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-title">Taux de R√©ussite</div>
            <div class="metric-value">{success_rate:.1f}%</div>
            <div class="metric-subtitle">{successful_count}/{total_count} mod√®les</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="metric-card {'best-model-card' if validation['best_model'] else ''}">
            <div class="metric-title">Meilleur Mod√®le</div>
            <div class="metric-value" style="font-size: 1.2rem;">{validation['best_model'] or 'N/A'}</div>
            <div class="metric-subtitle">Type: {validation['task_type'].title()}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        failed_count = len(validation["failed_models"])
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-title">√âchecs</div>
            <div class="metric-value" style="color: #e74c3c;">{failed_count}</div>
            <div class="metric-subtitle">Mod√®les √©chou√©s</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        memory_info = get_system_metrics()
        memory_color = "#27ae60" if memory_info['memory_percent'] < 70 else "#f39c12" if memory_info['memory_percent'] < 85 else "#e74c3c"
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-title">M√©moire Syst√®me</div>
            <div class="metric-value" style="color: {memory_color};">{memory_info['memory_percent']:.1f}%</div>
            <div class="metric-subtitle">Utilisation RAM</div>
        </div>
        """, unsafe_allow_html=True)

def create_pdf_report_latex(result, task_type):
    """G√©n√®re un rapport PDF avec LaTeX"""
    try:
        metrics = result.get('metrics', {})
        model_name = result.get('model_name', 'Unknown')
        training_time = result.get('training_time', 0)
        
        latex_content = r"""
\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{noto}
\begin{document}
\begin{center}
    \textbf{\Large Rapport d'√âvaluation du Mod√®le: """ + model_name + r"""}\\[0.5cm]
    \textit{Type de t√¢che: """ + task_type.title() + r"""}
\end{center}
\section*{M√©triques de Performance}
\begin{tabular}{ll}
    \toprule
    \textbf{M√©trique} & \textbf{Valeur} \\
    \midrule
"""
        if task_type == 'classification':
            latex_content += f"Accuracy & {metrics.get('accuracy', 0):.3f} \\\\\n"
            latex_content += f"Precision & {metrics.get('precision', 0):.3f} \\\\\n"
            latex_content += f"Recall & {metrics.get('recall', 0):.3f} \\\\\n"
            latex_content += f"F1-Score & {metrics.get('f1_score', 0):.3f} \\\\\n"
        elif task_type == 'regression':
            latex_content += f"R¬≤ Score & {metrics.get('r2', 0):.3f} \\\\\n"
            latex_content += f"MAE & {metrics.get('mae', 0):.3f} \\\\\n"
            latex_content += f"RMSE & {metrics.get('rmse', 0):.3f} \\\\\n"
        else:  # clustering
            latex_content += f"Silhouette Score & {metrics.get('silhouette_score', 0):.3f} \\\\\n"
            latex_content += f"Calinski-Harabasz & {metrics.get('calinski_harabasz', 0):.3f} \\\\\n"
            latex_content += f"Davies-Bouldin & {metrics.get('davies_bouldin_score', 0):.3f} \\\\\n"
            latex_content += f"Nombre de Clusters & {metrics.get('n_clusters', 'N/A')} \\\\\n"
        
        latex_content += f"Training Time & {training_time:.1f}s \\\\\n"
        latex_content += r"""
    \bottomrule
\end{tabular}
\section*{R√©sum√©}
Ce rapport pr√©sente les performances du mod√®le \textbf{""" + model_name + r"""} pour la t√¢che de """ + task_type + r""".
Veuillez consulter les visualisations pour plus de d√©tails.
\end{document}
"""
        return generate_pdf_report({'content': latex_content})
    except Exception as e:
        st.error(f"‚ùå Erreur g√©n√©ration PDF: {str(e)}")
        logger.error(f"PDF report generation failed: {e}")
        return None

@st.cache_data
def calculate_clustering_metrics_cached(X, labels):
    """Cache les calculs de m√©triques de clustering"""
    try:
        if X is None or labels is None:
            return {'error': 'Donn√©es manquantes'}
        evaluator = EvaluationMetrics(task_type='clustering')
        return evaluator.calculate_unsupervised_metrics(X, labels)
    except Exception as e:
        return {'error': str(e)}

def model_has_predict_proba(model):
    """V√©rifie si le mod√®le supporte predict_proba"""
    if model is None:
        return False
    if hasattr(model, 'named_steps'):
        final_step = list(model.named_steps.values())[-1]
        return hasattr(final_step, 'predict_proba')
    return hasattr(model, 'predict_proba')

def display_model_details(evaluator, model_result, task_type):
    """Affiche les d√©tails complets d'un mod√®le avec visualisations"""
    model_name = model_result.get('model_name', 'Unknown')
    st.markdown(f"#### D√©tails du mod√®le: {model_name}")

    with st.expander("üîç Debug - Donn√©es disponibles", expanded=False):
        available_keys = list(model_result.keys())
        st.write(f"**Cl√©s disponibles:** {', '.join(available_keys)}")
        data_status = {key: f"‚úÖ Pr√©sent ({getattr(model_result.get(key), 'shape', (len(model_result.get(key)),))})" 
                      if model_result.get(key) is not None and isinstance(model_result.get(key), (pd.DataFrame, pd.Series, np.ndarray)) 
                      else "‚úÖ Pr√©sent" if model_result.get(key) is not None else "‚ùå Manquant" 
                      for key in ['X_test', 'y_test', 'X_train', 'y_train', 'X_sample', 'labels', 'model']}
        st.json(data_status)

    metrics = model_result.get('metrics', {})
    training_time = model_result.get('training_time', 0)
    st.markdown("**M√©triques principales**")
    metrics_data = []

    if task_type == 'classification':
        metrics_data.extend([
            {'M√©trique': 'Accuracy', 'Valeur': f"{metrics.get('accuracy', 0):.3f}", 'Description': 'Proportion des pr√©dictions correctes'},
            {'M√©trique': 'Precision', 'Valeur': f"{metrics.get('precision', 0):.3f}", 'Description': 'Pr√©cision des pr√©dictions positives'},
            {'M√©trique': 'Recall', 'Valeur': f"{metrics.get('recall', 0):.3f}", 'Description': 'Rappel des vrais positifs'},
            {'M√©trique': 'F1-Score', 'Valeur': f"{metrics.get('f1_score', 0):.3f}", 'Description': 'Moyenne harmonique de pr√©cision et rappel'}
        ])
    elif task_type == 'regression':
        metrics_data.extend([
            {'M√©trique': 'R¬≤ Score', 'Valeur': f"{metrics.get('r2', 0):.3f}", 'Description': 'Coefficient de d√©termination'},
            {'M√©trique': 'MAE', 'Valeur': f"{metrics.get('mae', 0):.3f}", 'Description': 'Erreur absolue moyenne'},
            {'M√©trique': 'RMSE', 'Valeur': f"{metrics.get('rmse', 0):.3f}", 'Description': 'Racine de l\'erreur quadratique moyenne'}
        ])
    else:  # clustering
        metrics_data.extend([
            {'M√©trique': 'Silhouette Score', 'Valeur': f"{metrics.get('silhouette_score', 0):.3f}", 'Description': 'Qualit√© de la s√©paration des clusters'},
            {'M√©trique': 'Calinski-Harabasz', 'Valeur': f"{metrics.get('calinski_harabasz', 0):.3f}", 'Description': 'Ratio de dispersion entre et intra-clusters'},
            {'M√©trique': 'Davies-Bouldin', 'Valeur': f"{metrics.get('davies_bouldin_score', 0):.3f}", 'Description': 'Similitude moyenne entre clusters'},
            {'M√©trique': 'Nombre de Clusters', 'Valeur': f"{metrics.get('n_clusters', 'N/A')}", 'Description': 'Nombre de clusters form√©s'}
        ])
    metrics_data.append({'M√©trique': 'Temps d\'entra√Ænement (s)', 'Valeur': f"{training_time:.1f}", 'Description': 'Dur√©e de l\'entra√Ænement'})
    st.dataframe(pd.DataFrame(metrics_data), width='stretch')

    model = model_result.get('model')
    X_test = model_result.get('X_test')
    y_test = model_result.get('y_test')
    X_train = model_result.get('X_train')
    y_train = model_result.get('y_train')
    X_sample = model_result.get('X_sample')
    labels = model_result.get('labels')
    feature_names = model_result.get('feature_names', [])

    logger.info(f"üìä Affichage des d√©tails pour {model_name}, task_type={task_type}")

    if task_type in ['classification', 'regression'] and model:
        if feature_names:
            st.markdown("#### Importance des Features")
            try:
                feature_plot = evaluator.create_feature_importance_plot(model, feature_names)
                if feature_plot:
                    st.plotly_chart(feature_plot, width='stretch')
                else:
                    st.info("‚ÑπÔ∏è Importance des features non disponible pour ce type de mod√®le")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Erreur importance features: {str(e)[:100]}")
                logger.error(f"Erreur importance features {model_name}: {e}")

        if X_sample is not None and len(X_sample) > 0:
            st.markdown("#### Analyse SHAP")
            try:
                shap_plot = evaluator.create_shap_plot(model_result)
                if shap_plot:
                    st.plotly_chart(shap_plot, width='stretch')
                else:
                    st.info("‚ÑπÔ∏è Analyse SHAP non disponible pour ce mod√®le")
            except Exception as e:
                st.info(f"‚ÑπÔ∏è SHAP non disponible: {str(e)[:50]}")
                logger.info(f"SHAP non disponible pour {model_name}: {e}")

    if task_type == 'classification' and model:
        if X_test is not None and y_test is not None and len(X_test) > 0:
            st.markdown("#### Matrice de Confusion")
            try:
                cm_plot = evaluator.create_confusion_matrix_plot(model_result)
                if cm_plot:
                    st.plotly_chart(cm_plot, width='stretch')
                else:
                    st.warning("‚ö†Ô∏è Impossible d'afficher la matrice de confusion")
            except Exception as e:
                st.error(f"Erreur matrice de confusion: {str(e)[:100]}")
                logger.error(f"Erreur confusion matrix {model_name}: {e}")

            if model_has_predict_proba(model):
                st.markdown("#### Courbe ROC")
                try:
                    roc_plot = evaluator.create_roc_curve_plot(model_result)
                    if roc_plot:
                        st.plotly_chart(roc_plot, width='stretch')
                    else:
                        st.info("‚ÑπÔ∏è Courbe ROC disponible uniquement pour classification binaire")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Erreur courbe ROC: {str(e)[:100]}")
                    logger.warning(f"√âchec ROC pour {model_name}: {e}")

                st.markdown("#### Courbe de Pr√©cision-Rappel")
                try:
                    pr_plot = evaluator.create_precision_recall_curve_plot(model_result)
                    if pr_plot:
                        st.plotly_chart(pr_plot, width='stretch')
                    else:
                        st.info("‚ÑπÔ∏è Courbe PR disponible uniquement pour classification binaire")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Erreur courbe PR: {str(e)[:100]}")

                st.markdown("#### Distribution des Probabilit√©s Pr√©dites")
                try:
                    proba_plot = evaluator.create_predicted_proba_distribution_plot(model_result)
                    if proba_plot:
                        st.plotly_chart(proba_plot, width='stretch')
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Erreur distribution probabilit√©s: {str(e)[:100]}")
            else:
                st.info("‚ÑπÔ∏è Mod√®le ne supporte pas predict_proba - courbes ROC/PR non disponibles")

        if X_train is not None and y_train is not None and len(X_train) > 0:
            with st.expander("#### üìà Courbe d'Apprentissage", expanded=False):
                try:
                    learning_plot = evaluator.create_learning_curve_plot(model_result)
                    if learning_plot:
                        st.plotly_chart(learning_plot, width='stretch')
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Erreur courbe d'apprentissage: {str(e)[:100]}")

        if X_sample is not None and len(X_sample) > 0:
            with st.expander("#### üî• Heatmap de Corr√©lation des Features", expanded=False):
                try:
                    temp_result = {**model_result, 'X_train': X_sample}
                    corr_plot = evaluator.create_feature_correlation_heatmap(temp_result)
                    if corr_plot:
                        st.plotly_chart(corr_plot, width='stretch')
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Erreur heatmap: {str(e)[:100]}")

    if task_type == 'regression' and model:
        if X_test is not None and y_test is not None and len(X_test) > 0:
            st.markdown("#### Graphique des R√©sidus")
            try:
                residuals_plot = evaluator.create_residuals_plot(model_result)
                if residuals_plot:
                    st.plotly_chart(residuals_plot, width='stretch')
            except Exception as e:
                st.error(f"Erreur graphique r√©sidus: {str(e)[:100]}")

            st.markdown("#### Pr√©dictions vs. R√©elles")
            try:
                pred_vs_actual_plot = evaluator.create_predicted_vs_actual_plot(model_result)
                if pred_vs_actual_plot:
                    st.plotly_chart(pred_vs_actual_plot, width='stretch')
            except Exception as e:
                st.error(f"Erreur pr√©dictions vs. r√©elles: {str(e)[:100]}")

        if X_train is not None and y_train is not None and len(X_train) > 0:
            with st.expander("#### üìà Courbe d'Apprentissage", expanded=False):
                try:
                    learning_plot = evaluator.create_learning_curve_plot(model_result)
                    if learning_plot:
                        st.plotly_chart(learning_plot, width='stretch')
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Erreur courbe d'apprentissage: {str(e)[:100]}")

    if task_type == 'clustering':
        if X_sample is not None and labels is not None and len(X_sample) > 0:
            st.markdown("#### Visualisation des Clusters")
            try:
                cluster_plot = evaluator.create_cluster_scatter_plot(model_result)
                if cluster_plot:
                    st.plotly_chart(cluster_plot, width='stretch')
            except Exception as e:
                st.error(f"Erreur scatter plot clusters: {str(e)[:100]}")
                logger.error(f"Erreur cluster scatter {model_name}: {e}")

            st.markdown("#### Analyse de Silhouette")
            try:
                silhouette_plot = evaluator.create_silhouette_plot(model_result)
                if silhouette_plot:
                    st.plotly_chart(silhouette_plot, width='stretch')
            except Exception as e:
                st.error(f"Erreur silhouette plot: {str(e)[:100]}")
                logger.error(f"Erreur silhouette {model_name}: {e}")

            st.markdown("#### Dispersion Intra-Cluster")
            try:
                intra_cluster_plot = evaluator.create_intra_cluster_distance_plot(model_result)
                if intra_cluster_plot:
                    st.plotly_chart(intra_cluster_plot, width='stretch')
            except Exception as e:
                st.error(f"Erreur dispersion intra-cluster: {str(e)[:100]}")
                logger.error(f"Erreur intra-cluster {model_name}: {e}")
        else:
            st.warning("‚ö†Ô∏è Donn√©es de clustering (X_sample, labels) manquantes pour les visualisations")
            logger.warning(f"X_sample ou labels manquant pour {model_name}")

    st.markdown("---")
    cv_scores = model_result.get('cv_scores')
    if cv_scores is not None and isinstance(cv_scores, (list, np.ndarray)) and len(cv_scores) > 0:
        cv_mean = np.mean(cv_scores)
        cv_std = np.std(cv_scores)
        st.caption(f"üìä Validation Crois√©e: {cv_mean:.3f} ¬± {cv_std:.3f}")
    st.caption(f"üìä Visualisations g√©n√©r√©es pour {model_name} | ‚è±Ô∏è Entra√Ænement: {training_time:.2f}s")

def get_mlflow_artifact(run_id, artifact_path, client):
    """R√©cup√®re un artefact MLflow"""
    try:
        return client.download_artifacts(run_id, artifact_path)
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration artefact MLflow {run_id}: {e}")
        return None

def create_mlflow_run_plot(runs, task_type):
    """Cr√©e un graphique de comparaison des runs MLflow"""
    try:
        # V√©rification que task_type est une cha√Æne
        if isinstance(task_type, list):
            logger.warning("task_type est une liste, utilisation de la premi√®re valeur ou 'classification' par d√©faut")
            task_type = task_type[0] if task_type else 'classification'
        
        metric_key = 'metrics.accuracy' if task_type == 'classification' else 'metrics.r2' if task_type == 'regression' else 'metrics.silhouette_score'
        metric_label = 'Accuracy' if task_type == 'classification' else 'R¬≤ Score' if task_type == 'regression' else 'Silhouette Score'

        plot_data = [
            {'Model': run.get('tags.mlflow.runName', 'Unknown'), 'Metric': float(run.get(metric_key, 0)), 'Run ID': run.get('run_id', 'N/A')}
            for run in runs if run.get(metric_key) is not None
        ]

        if not plot_data:
            return None

        df_plot = pd.DataFrame(plot_data)
        fig = px.bar(
            df_plot,
            x='Model',
            y='Metric',
            color='Metric',
            text='Metric',
            title=f"Comparaison des Runs MLflow ({metric_label})",
            hover_data=['Run ID'],
            color_continuous_scale='Viridis'
        )
        fig.update_traces(texttemplate='%{text:.3f}', textposition='auto')
        fig.update_layout(xaxis_title="Mod√®le", yaxis_title=metric_label, showlegend=False, height=400)
        return fig
    except Exception as e:
        logger.error(f"Erreur cr√©ation graphique MLflow: {e}")
        return None

def display_mlflow_tab():
    """Affiche l'onglet MLflow avec gestion d'erreurs robuste"""
    st.markdown('<div class="tab-content">', unsafe_allow_html=True)
    st.markdown("### üîó Exploration des Runs MLflow")

    if not MLFLOW_AVAILABLE:
        st.error("üö´ MLflow non disponible")
        st.info("Installez MLflow pour acc√©der aux runs (`pip install mlflow`).")
        st.markdown('</div>', unsafe_allow_html=True)
        return

    if 'mlflow_runs' not in st.session_state or st.session_state.mlflow_runs is None:
        st.session_state.mlflow_runs = []
        st.warning("‚ö†Ô∏è Aucun run MLflow disponible")
        st.info("Entra√Ænez des mod√®les dans 'Configuration ML' pour g√©n√©rer des runs.")
        if st.button("üîÑ Initialiser MLflow runs"):
            st.session_state.mlflow_runs = []
            st.success("‚úÖ mlflow_runs initialis√©")
            st.rerun()
        st.markdown('</div>', unsafe_allow_html=True)
        return

    mlflow_runs = st.session_state.mlflow_runs
    if not isinstance(mlflow_runs, list):
        st.error(f"‚ùå Format des runs MLflow invalide: {type(mlflow_runs)}")
        logger.error(f"mlflow_runs type incorrect: {type(mlflow_runs)}")
        if st.button("üîÑ Corriger le format"):
            st.session_state.mlflow_runs = []
            st.success("‚úÖ Format corrig√©")
            st.rerun()
        st.markdown('</div>', unsafe_allow_html=True)
        return

    if not mlflow_runs:
        st.warning("‚ö†Ô∏è Liste des runs MLflow vide")
        st.info("Entra√Ænez des mod√®les pour g√©n√©rer des runs.")
        st.markdown('</div>', unsafe_allow_html=True)
        return

    first_run = mlflow_runs[0]
    required_keys = ['run_id', 'status', 'start_time']
    missing_keys = [key for key in required_keys if key not in first_run]
    if missing_keys:
        st.error(f"‚ùå Cl√©s manquantes dans les runs: {missing_keys}")
        st.json({"keys_disponibles": list(first_run.keys())})
        logger.error(f"Run structure invalide. Keys: {list(first_run.keys())}")
        st.markdown('</div>', unsafe_allow_html=True)
        return

    st.markdown(f"**üìä {len(mlflow_runs)} runs MLflow disponibles**")
    col1, col2, col3 = st.columns(3)
    with col1:
        finished_runs = len([r for r in mlflow_runs if r.get('status') == 'FINISHED'])
        st.metric("Runs R√©ussis", finished_runs)
    with col2:
        failed_runs = len([r for r in mlflow_runs if r.get('status') == 'FAILED'])
        st.metric("Runs √âchou√©s", failed_runs)
    with col3:
        if finished_runs > 0:
            success_rate = (finished_runs / len(mlflow_runs)) * 100
            st.metric("Taux de R√©ussite", f"{success_rate:.1f}%")

    st.markdown("#### üìã Filtrer les Runs")
    col_filter1, col_filter2, col_filter3 = st.columns(3)
    with col_filter1:
        run_status = st.multiselect("Statut du Run", options=['FINISHED', 'FAILED', 'RUNNING'], default=['FINISHED'], key="mlflow_status_filter")
    with col_filter2:
        model_names = sorted(set(run.get('tags.mlflow.runName', 'Unknown').split('_')[0] for run in mlflow_runs))
        selected_models = st.multiselect("Mod√®les", options=model_names, default=model_names, key="mlflow_model_filter")
    with col_filter3:
        available_metrics = set(k.split('.')[-1] for run in mlflow_runs for k in run.keys() if k.startswith('metrics.'))
        sort_metric = st.selectbox("Trier par", options=['start_time'] + list(available_metrics), index=0, key="mlflow_sort_by")

    filtered_runs = [run for run in mlflow_runs if run.get('status', 'UNKNOWN') in run_status and run.get('tags.mlflow.runName', 'Unknown').split('_')[0] in selected_models]
    filtered_runs = sorted(filtered_runs, key=lambda x: x.get(f'metrics.{sort_metric}', x.get('start_time', 0)), reverse=sort_metric != 'start_time')
    st.markdown(f"**{len(filtered_runs)} runs filtr√©s**")

    if filtered_runs:
        run_data = []
        for run in filtered_runs:
            metrics = {k.split('.')[-1]: v for k, v in run.items() if k.startswith('metrics.')}
            params = {k.split('.')[-1]: v for k, v in run.items() if k.startswith('params.')}
            run_id = run.get('run_id', 'N/A')
            row = {
                'Run ID': run_id[:8] + '...' if len(run_id) > 8 else run_id,
                'Mod√®le': run.get('tags.mlflow.runName', 'Unknown'),
                'Statut': run.get('status', 'UNKNOWN'),
                'Date': pd.to_datetime(run.get('start_time', 0), unit='ms').strftime('%Y-%m-%d %H:%M:%S'),
                **{k.title(): f"{v:.3f}" if isinstance(v, (int, float)) else v for k, v in metrics.items() if k in ['accuracy', 'f1_score', 'r2', 'mae', 'silhouette_score']}
            }
            run_data.append(row)

        df_runs = pd.DataFrame(run_data)
        st.markdown("#### üìä Tableau des Runs MLflow")
        st.dataframe(df_runs, width='stretch', height=400)

        st.markdown("#### üìà Comparaison des Performances")
        try:
            run_plot = create_mlflow_run_plot(filtered_runs, st.session_state.ml_results.get('task_type', 'classification'))
            if run_plot:
                st.plotly_chart(cached_plot(run_plot, "mlflow_run_plot"), width='stretch')
            else:
                st.warning("‚ö†Ô∏è Impossible de g√©n√©rer le graphique de comparaison")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur graphique: {str(e)[:100]}")
            logger.warning(f"√âchec cr√©ation graphique MLflow: {e}")

        st.markdown("#### üîç D√©tails du Run")
        selected_run_idx = st.selectbox(
            "S√©lectionner un Run",
            options=range(len(filtered_runs)),
            format_func=lambda x: f"{filtered_runs[x].get('tags.mlflow.runName', 'Unknown')} ({filtered_runs[x].get('status', 'UNKNOWN')})",
            key="mlflow_run_selector"
        )
        if selected_run_idx is not None:
            selected_run = filtered_runs[selected_run_idx]
            st.markdown("**Informations du Run**")
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**M√©triques**")
                metrics = {k.split('.')[-1]: v for k, v in selected_run.items() if k.startswith('metrics.')}
                st.json(metrics if metrics else {"info": "Aucune m√©trique disponible"})
            with col2:
                st.markdown("**Param√®tres**")
                params = {k.split('.')[-1]: v for k, v in selected_run.items() if k.startswith('params.')}
                display_params = dict(list(params.items())[:20]) if params else {"info": "Aucun param√®tre disponible"}
                st.json(display_params)
                if len(params) > 20:
                    st.caption(f"... et {len(params) - 20} autres param√®tres")
            with st.expander("üìã Informations Compl√®tes du Run", expanded=False):
                st.json(selected_run)

            if st.button("üì• T√©l√©charger Artefacts", key=f"download_artifacts_{selected_run.get('run_id')}"):
                artifact_data = get_mlflow_artifact(selected_run.get('run_id'), "model", MlflowClient())
                if artifact_data:
                    st.success("‚úÖ Artefact t√©l√©charg√©!")
                else:
                    st.error("‚ùå Erreur lors du t√©l√©chargement des artefacts")

        if st.button("üì• T√©l√©charger Runs CSV", key="download_mlflow_csv"):
            try:
                csv_data = pd.DataFrame(run_data).to_csv(index=False)
                st.download_button(
                    label="üíæ T√©l√©charger CSV",
                    data=csv_data,
                    file_name=f"mlflow_runs_{int(time.time())}.csv",
                    mime="text/csv",
                    width='stretch'
                )
            except Exception as e:
                st.error(f"Erreur export CSV: {str(e)}")
    else:
        st.info("Aucun run ne correspond aux filtres s√©lectionn√©s")
    st.markdown('</div>', unsafe_allow_html=True)

def main():
    """Fonction principale de la page d'√©valuation"""
    if 'ml_results' not in st.session_state or not st.session_state.ml_results:
        st.error("üö´ Aucun r√©sultat disponible")
        st.info("Entra√Ænez des mod√®les dans 'Configuration ML'.")
        if st.button("‚öôÔ∏è Aller √† la page entrainement", width='stretch'):
            st.switch_page("pages/2_training.py")
        return

    try:
        evaluator = ModelEvaluationVisualizer(st.session_state.ml_results)
        validation = evaluator.validation_result
    except Exception as e:
        st.error(f"‚ùå Erreur initialisation: {str(e)}")
        st.info("Action: V√©rifiez les r√©sultats ou relancez l'entra√Ænement.")
        logger.error(f"Erreur initialisation visualizer: {e}")
        return

    if not validation["has_results"]:
        st.error("üì≠ Aucune donn√©e valide")
        return

    display_metrics_header(validation)
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["üìä Vue d'Ensemble", "üîç D√©tails", "üìà M√©triques", "üíæ Export", "üîó MLflow"])

    with tab1:
        st.markdown('<div class="tab-content">', unsafe_allow_html=True)
        if validation["successful_models"]:
            st.markdown("### üìà Comparaison des Performances")
            comparison_plot = evaluator.get_comparison_plot()
            if comparison_plot:
                st.plotly_chart(cached_plot(comparison_plot, "comparison_plot"), width='stretch')
            
            st.markdown("### üìã Synth√®se")
            df_comparison = evaluator.get_comparison_dataframe()
            st.dataframe(df_comparison, width='stretch', height=400)
            
            st.markdown("### üìä R√©sum√© Statistique")
            col1, col2, col3 = st.columns(3)
            if validation["task_type"] in ['classification', 'regression']:
                numeric_cols = df_comparison.select_dtypes(include=[np.number])
                if not numeric_cols.empty:
                    main_metric = 'Accuracy' if validation["task_type"] == 'classification' else 'R¬≤'
                    if main_metric in numeric_cols.columns:
                        with col1:
                            st.metric("Score Moyen", f"{numeric_cols[main_metric].mean():.3f}")
                        with col2:
                            st.metric("Meilleur Score", f"{numeric_cols[main_metric].max():.3f}")
                        with col3:
                            st.metric("√âcart-type", f"{numeric_cols[main_metric].std():.3f}")
            elif validation["task_type"] == 'clustering':
                with col1:
                    avg_silhouette = np.mean([r.get('metrics', {}).get('silhouette_score', 0) for r in validation["successful_models"]])
                    st.metric("Silhouette Moyen", f"{avg_silhouette:.3f}")
                with col2:
                    avg_clusters = np.mean([r.get('metrics', {}).get('n_clusters', 0) for r in validation["successful_models"]])
                    st.metric("Clusters Moyen", f"{avg_clusters:.1f}")
                with col3:
                    best_silhouette = max([r.get('metrics', {}).get('silhouette_score', 0) for r in validation["successful_models"]])
                    st.metric("Meilleur Silhouette", f"{best_silhouette:.3f}")
        else:
            st.warning("‚ö†Ô∏è Aucun mod√®le valide")
        st.markdown('</div>', unsafe_allow_html=True)

    with tab2:
        st.markdown('<div class="tab-content">', unsafe_allow_html=True)
        st.markdown("### üîç Analyse D√©taill√©e")
        if validation["successful_models"]:
            model_names = [r.get('model_name', f'Mod√®le_{i}') for i, r in enumerate(validation["successful_models"])]
            selected_idx = st.selectbox(
                "Mod√®le √† analyser:",
                range(len(model_names)),
                format_func=lambda x: f"{model_names[x]} {'üèÜ' if model_names[x] == validation.get('best_model') else ''}",
                key="model_selector_detail"
            )
            model_result = validation["successful_models"][selected_idx]
            display_model_details(evaluator, model_result, validation["task_type"])
        else:
            st.info("‚ÑπÔ∏è Aucun mod√®le disponible")
        st.markdown('</div>', unsafe_allow_html=True)

    with tab3:
        st.markdown('<div class="tab-content">', unsafe_allow_html=True)
        st.markdown("### üìà M√©triques Avanc√©es")
        if validation["successful_models"]:
            st.markdown("#### üìä Distribution des Performances")
            dist_plot = evaluator.get_performance_distribution_plot()
            if dist_plot:
                st.plotly_chart(cached_plot(dist_plot, "dist_plot"), width='stretch')
            
            st.markdown("#### üìã M√©triques par Cat√©gorie")
            if validation["task_type"] == 'clustering':
                st.markdown("**üéØ Qualit√© des Clusters**")
                clustering_metrics = [
                    {
                        'Mod√®le': r.get('model_name', 'Unknown'),
                        'Silhouette': f"{r.get('metrics', {}).get('silhouette_score', 0):.3f}",
                        'Calinski-Harabasz': f"{r.get('metrics', {}).get('calinski_harabasz', 0):.3f}",
                        'Davies-Bouldin': f"{r.get('metrics', {}).get('davies_bouldin_score', 0):.3f}",
                        'Clusters': r.get('metrics', {}).get('n_clusters', 'N/A'),
                        'Qualit√©': 'üü¢ Excellente' if r.get('metrics', {}).get('silhouette_score', 0) > 0.7 else 'üü° Bonne' if r.get('metrics', {}).get('silhouette_score', 0) > 0.5 else 'üü† Moyenne' if r.get('metrics', {}).get('silhouette_score', 0) > 0.3 else 'üî¥ Faible'
                    }
                    for r in validation["successful_models"]
                ]
                st.dataframe(pd.DataFrame(clustering_metrics), width='stretch')
                
                st.markdown("**‚è±Ô∏è Performance Computationnelle**")
                perf_metrics = [{'Mod√®le': r.get('model_name'), 'Temps (s)': f"{r.get('training_time', 0):.1f}"} for r in validation["successful_models"]]
                st.dataframe(pd.DataFrame(perf_metrics), width='stretch')
            
            elif validation["task_type"] == 'classification':
                st.markdown("**üè∑Ô∏è Pr√©cision**")
                class_metrics = [
                    {
                        'Mod√®le': r.get('model_name', 'Unknown'),
                        'Accuracy': f"{r.get('metrics', {}).get('accuracy', 0):.3f}",
                        'Precision': f"{r.get('metrics', {}).get('precision', 0):.3f}",
                        'Recall': f"{r.get('metrics', {}).get('recall', 0):.3f}",
                        'F1-Score': f"{r.get('metrics', {}).get('f1_score', 0):.3f}"
                    }
                    for r in validation["successful_models"]
                ]
                st.dataframe(pd.DataFrame(class_metrics), width='stretch')
                
                st.markdown("**‚è±Ô∏è Performance Computationnelle**")
                perf_metrics = [{'Mod√®le': r.get('model_name'), 'Temps (s)': f"{r.get('training_time', 0):.1f}"} for r in validation["successful_models"]]
                st.dataframe(pd.DataFrame(perf_metrics), width='stretch')
            
            elif validation["task_type"] == 'regression':
                st.markdown("**üìä Pr√©cision**")
                reg_metrics = [
                    {
                        'Mod√®le': r.get('model_name', 'Unknown'),
                        'R¬≤ Score': f"{r.get('metrics', {}).get('r2', 0):.3f}",
                        'MAE': f"{r.get('metrics', {}).get('mae', 0):.3f}",
                        'RMSE': f"{r.get('metrics', {}).get('rmse', 0):.3f}"
                    }
                    for r in validation["successful_models"]
                ]
                st.dataframe(pd.DataFrame(reg_metrics), width='stretch')
                
                st.markdown("**‚è±Ô∏è Performance Computationnelle**")
                perf_metrics = [{'Mod√®le': r.get('model_name'), 'Temps (s)': f"{r.get('training_time', 0):.1f}"} for r in validation["successful_models"]]
                st.dataframe(pd.DataFrame(perf_metrics), width='stretch')
        else:
            st.warning("‚ö†Ô∏è Aucune m√©trique disponible")
        st.markdown('</div>', unsafe_allow_html=True)

    with tab4:
        st.markdown('<div class="tab-content">', unsafe_allow_html=True)
        st.markdown("### üíæ Export des R√©sultats")
        if validation["successful_models"]:
            export_data = evaluator.get_export_data()
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("#### üìä Donn√©es Structur√©es")
                csv_data = pd.DataFrame(export_data['models']).to_csv(index=False)
                st.download_button(
                    label="üì• T√©l√©charger CSV",
                    data=csv_data,
                    file_name=f"evaluation_{validation['task_type']}_{int(time.time())}.csv",
                    mime="text/csv",
                    width='stretch'
                )
                st.download_button(
                    label="üì• T√©l√©charger JSON",
                    data=json.dumps(export_data, indent=2, ensure_ascii=False, default=str),
                    file_name=f"evaluation_{validation['task_type']}_{int(time.time())}.json",
                    mime="application/json",
                    width='stretch'
                )
                with st.expander("üëÅÔ∏è Aper√ßu"):
                    st.json(export_data, expanded=False)
            with col2:
                st.markdown("#### üìà Rapport Global")
                if validation["best_model"]:
                    best_model_result = next((r for r in validation["successful_models"] if r.get('model_name') == validation["best_model"]), None)
                    if best_model_result:
                        pdf_bytes = create_pdf_report_latex(best_model_result, validation["task_type"])
                        if pdf_bytes:
                            st.download_button(
                                label="üìÑ Rapport PDF",
                                data=pdf_bytes,
                                file_name=f"rapport_{validation['best_model']}_{int(time.time())}.pdf",
                                mime="application/pdf",
                                width='stretch'
                            )
                if st.button("üîÑ G√©n√©rer Rapport", width='stretch'):
                    with st.spinner("G√©n√©ration..."):
                        time.sleep(1)
                        st.success("‚úÖ Rapport g√©n√©r√©!")
                        st.balloons()
        else:
            st.info("‚ÑπÔ∏è Aucune donn√©e disponible")
        st.markdown('</div>', unsafe_allow_html=True)

    with tab5:
        display_mlflow_tab()

if __name__ == "__main__":
    main()