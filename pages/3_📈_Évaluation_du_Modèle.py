import os
import numpy as np
import streamlit as st
import pandas as pd
import time
import json
from ml.evaluation.visualization import ModelEvaluationVisualizer
from ml.evaluation.metrics_calculation import get_system_metrics, EvaluationMetrics
from utils.report_generator import generate_pdf_report

from logging import getLogger
logger = getLogger(__name__)

# Configuration PyArrow
os.environ["PANDAS_USE_PYARROW"] = "0"
try:
    pd.options.mode.dtype_backend = "numpy_nullable"
except Exception:
    pass

# Configuration page
st.set_page_config(page_title="√âvaluation des Mod√®les", page_icon="üìà", layout="wide", initial_sidebar_state="expanded")

# CSS personnalis√©
st.markdown("""
<style>
    .main-header {
        font-size: 2rem;
        color: #2c3e50;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: 600;
        border-bottom: 2px solid #3498db;
        padding-bottom: 1rem;
    }
    .metric-card {
        background: #ffffff;
        border: 1px solid #e1e8ed;
        border-radius: 8px;
        padding: 1rem;
        margin: 0.5rem 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.06);
    }
    .best-model-card {
        border-left: 4px solid #27ae60;
        background: linear-gradient(135deg, #f8fff9 0%, #e8f5e8 100%);
    }
    .metric-title {
        font-size: 0.8rem;
        color: #7f8c8d;
        font-weight: 600;
        text-transform: uppercase;
    }
    .metric-value {
        font-size: 1.6rem;
        font-weight: 700;
        color: #2c3e50;
    }
    .metric-subtitle {
        font-size: 0.7rem;
        color: #95a5a6;
    }
    .performance-high { color: #27ae60; background: #d5f4e6; padding: 2px 6px; border-radius: 4px; }
    .performance-medium { color: #f39c12; background: #fef9e7; padding: 2px 6px; border-radius: 4px; }
    .performance-low { color: #e74c3c; background: #fadbd8; padding: 2px 6px; border-radius: 4px; }
    .tab-content {
        padding: 1rem;
        background: #ffffff;
        border-radius: 8px;
        border: 1px solid #ecf0f1;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_data(ttl=3600, max_entries=5)
def cached_plot(fig, _size_key: str):
    """Cache les figures Plotly avec limite de taille"""
    if fig and hasattr(fig, 'to_json'):
        size_mb = len(json.dumps(fig.to_json())) / (1024**2)
        if size_mb > 10:
            st.warning("‚ö†Ô∏è Graphique volumineux, affichage simplifi√©")
            return None
    return fig

def display_metrics_header(validation):
    """Affiche l'en-t√™te avec m√©triques principales"""
    successful_count = len(validation["successful_models"])
    total_count = validation["results_count"]
    
    st.markdown('<div class="main-header">üìà √âvaluation des Mod√®les</div>', unsafe_allow_html=True)
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        success_rate = (successful_count / total_count * 100) if total_count > 0 else 0
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-title">Taux de R√©ussite</div>
            <div class="metric-value">{success_rate:.1f}%</div>
            <div class="metric-subtitle">{successful_count}/{total_count} mod√®les</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="metric-card {'best-model-card' if validation['best_model'] else ''}">
            <div class="metric-title">Meilleur Mod√®le</div>
            <div class="metric-value" style="font-size: 1.2rem;">{validation['best_model'] or 'N/A'}</div>
            <div class="metric-subtitle">Type: {validation['task_type'].title()}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        failed_count = len(validation["failed_models"])
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-title">√âchecs</div>
            <div class="metric-value" style="color: #e74c3c;">{failed_count}</div>
            <div class="metric-subtitle">Mod√®les √©chou√©s</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        memory_info = get_system_metrics()
        memory_color = "#27ae60" if memory_info['memory_percent'] < 70 else "#f39c12" if memory_info['memory_percent'] < 85 else "#e74c3c"
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-title">M√©moire Syst√®me</div>
            <div class="metric-value" style="color: {memory_color};">{memory_info['memory_percent']:.1f}%</div>
            <div class="metric-subtitle">Utilisation RAM</div>
        </div>
        """, unsafe_allow_html=True)

def create_pdf_report_latex(result, task_type):
    """G√©n√®re un rapport PDF avec LaTeX"""
    try:
        metrics = result.get('metrics', {})
        model_name = result.get('model_name', 'Unknown')
        training_time = result.get('training_time', 0)
        
        latex_content = r"""
\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{noto}
\begin{document}
\begin{center}
    \textbf{\Large Rapport d'√âvaluation du Mod√®le: """ + model_name + r"""}\\[0.5cm]
    \textit{Type de t√¢che: """ + task_type.title() + r"""}
\end{center}
\section*{M√©triques de Performance}
\begin{tabular}{ll}
    \toprule
    \textbf{M√©trique} & \textbf{Valeur} \\
    \midrule
"""
        if task_type == 'classification':
            latex_content += f"Accuracy & {metrics.get('accuracy', 0):.3f} \\\\\n"
            latex_content += f"Precision & {metrics.get('precision', 0):.3f} \\\\\n"
            latex_content += f"Recall & {metrics.get('recall', 0):.3f} \\\\\n"
            latex_content += f"F1-Score & {metrics.get('f1_score', 0):.3f} \\\\\n"
        elif task_type == 'regression':
            latex_content += f"R¬≤ Score & {metrics.get('r2', 0):.3f} \\\\\n"
            latex_content += f"MAE & {metrics.get('mae', 0):.3f} \\\\\n"
            latex_content += f"RMSE & {metrics.get('rmse', 0):.3f} \\\\\n"
        else:  # clustering
            latex_content += f"Silhouette Score & {metrics.get('silhouette_score', 0):.3f} \\\\\n"
            latex_content += f"Calinski-Harabasz & {metrics.get('calinski_harabasz', 0):.3f} \\\\\n"
            latex_content += f"Davies-Bouldin & {metrics.get('davies_bouldin_score', 0):.3f} \\\\\n"
            latex_content += f"Nombre de Clusters & {metrics.get('n_clusters', 'N/A')} \\\\\n"
        
        latex_content += f"Training Time & {training_time:.1f}s \\\\\n"
        latex_content += r"""
    \bottomrule
\end{tabular}
\section*{R√©sum√©}
Ce rapport pr√©sente les performances du mod√®le \textbf{""" + model_name + r"""} pour la t√¢che de """ + task_type + r""".
Veuillez consulter les visualisations pour plus de d√©tails.
\end{document}
"""
        return generate_pdf_report({'content': latex_content})
    except Exception as e:
        st.error(f"‚ùå Erreur g√©n√©ration PDF: {str(e)}")
        return None

@st.cache_data
def calculate_clustering_metrics_cached(X, labels):
    """Cache les calculs de m√©triques de clustering"""
    try:
        if X is None or labels is None:
            return {'error': 'Donn√©es manquantes'}
        evaluator = EvaluationMetrics(task_type='clustering')
        return evaluator.calculate_unsupervised_metrics(X, labels)
    except Exception as e:
        return {'error': str(e)}

def display_model_details(evaluator, model_result, task_type):
    """Affiche les d√©tails d'un mod√®le s√©lectionn√©"""
    st.markdown(f"#### D√©tails du mod√®le: {model_result.get('model_name', 'Unknown')}")
    
    # Afficher les m√©triques
    metrics = model_result.get('metrics', {})
    training_time = model_result.get('training_time', 0)
    
    st.markdown("**M√©triques principales**")
    metrics_data = []
    if task_type == 'classification':
        metrics_data.append({
            'M√©trique': 'Accuracy', 'Valeur': f"{metrics.get('accuracy', 0):.3f}",
            'Description': 'Proportion des pr√©dictions correctes'
        })
        metrics_data.append({
            'M√©trique': 'Precision', 'Valeur': f"{metrics.get('precision', 0):.3f}",
            'Description': 'Pr√©cision des pr√©dictions positives'
        })
        metrics_data.append({
            'M√©trique': 'Recall', 'Valeur': f"{metrics.get('recall', 0):.3f}",
            'Description': 'Rappel des vrais positifs'
        })
        metrics_data.append({
            'M√©trique': 'F1-Score', 'Valeur': f"{metrics.get('f1_score', 0):.3f}",
            'Description': 'Moyenne harmonique de pr√©cision et rappel'
        })
    elif task_type == 'regression':
        metrics_data.append({
            'M√©trique': 'R¬≤ Score', 'Valeur': f"{metrics.get('r2', 0):.3f}",
            'Description': 'Coefficient de d√©termination'
        })
        metrics_data.append({
            'M√©trique': 'MAE', 'Valeur': f"{metrics.get('mae', 0):.3f}",
            'Description': 'Erreur absolue moyenne'
        })
        metrics_data.append({
            'M√©trique': 'RMSE', 'Valeur': f"{metrics.get('rmse', 0):.3f}",
            'Description': 'Racine de l‚Äôerreur quadratique moyenne'
        })
    else:  # clustering
        metrics_data.append({
            'M√©trique': 'Silhouette Score', 'Valeur': f"{metrics.get('silhouette_score', 0):.3f}",
            'Description': 'Qualit√© de la s√©paration des clusters'
        })
        metrics_data.append({
            'M√©trique': 'Calinski-Harabasz', 'Valeur': f"{metrics.get('calinski_harabasz', 0):.3f}",
            'Description': 'Ratio de dispersion entre et intra-clusters'
        })
        metrics_data.append({
            'M√©trique': 'Davies-Bouldin', 'Valeur': f"{metrics.get('davies_bouldin_score', 0):.3f}",
            'Description': 'Similitude moyenne entre clusters'
        })
        metrics_data.append({
            'M√©trique': 'Nombre de Clusters', 'Valeur': f"{metrics.get('n_clusters', 'N/A')}",
            'Description': 'Nombre de clusters form√©s'
        })
    
    metrics_data.append({
        'M√©trique': 'Temps d‚Äôentra√Ænement (s)', 'Valeur': f"{training_time:.1f}",
        'Description': 'Dur√©e de l‚Äôentra√Ænement'
    })
    
    st.dataframe(pd.DataFrame(metrics_data), width='stretch')

    # Visualisations sp√©cifiques
    model_name = model_result.get('model_name', 'Unknown')
    model = model_result.get('model')
    logger.info(f"üìä Affichage des d√©tails pour {model_name}, task_type={task_type}")

    # Importance des features (classification et r√©gression)
    if task_type in ['classification', 'regression'] and model and model_result.get('feature_names'):
        st.markdown("#### Importance des Features")
        feature_plot = evaluator.create_feature_importance_plot(model, model_result['feature_names'])
        if feature_plot:
            st.plotly_chart(feature_plot, width='stretch')
        else:
            st.warning("‚ö†Ô∏è Impossible d‚Äôafficher l‚Äôimportance des features")
            logger.warning(f"‚ö†Ô∏è √âchec cr√©ation importance des features pour {model_name}")

    # SHAP Summary Plot (classification et r√©gression)
    if task_type in ['classification', 'regression'] and model and model_result.get('X_sample') is not None and not model_result.get('X_sample').empty:
        st.markdown("#### Analyse SHAP")
        shap_plot = evaluator.create_shap_plot(model_result)
        if shap_plot:
            st.plotly_chart(shap_plot, width='stretch')
        else:
            st.warning("‚ö†Ô∏è Impossible d‚Äôafficher le SHAP plot")
            logger.warning(f"‚ö†Ô∏è √âchec cr√©ation SHAP plot pour {model_name}")

    # Nouvelles visualisations pour classification
    if task_type == 'classification' and model and model_result.get('X_test') is not None and model_result.get('y_test') is not None:
        # Matrice de confusion
        st.markdown("#### Matrice de Confusion")
        cm_plot = evaluator.create_confusion_matrix_plot(model_result)
        if cm_plot:
            st.plotly_chart(cm_plot, width='stretch')
        else:
            st.warning("‚ö†Ô∏è Impossible d‚Äôafficher la matrice de confusion")
            logger.warning(f"‚ö†Ô∏è √âchec cr√©ation matrice de confusion pour {model_name}: X_test={model_result.get('X_test') is not None}, y_test={model_result.get('y_test') is not None}")

        # Courbe ROC
        if hasattr(model, 'predict_proba'):
            st.markdown("#### Courbe ROC")
            roc_plot = evaluator.create_roc_curve_plot(model_result)
            if roc_plot:
                st.plotly_chart(roc_plot, width='stretch')
            else:
                st.warning("‚ö†Ô∏è Impossible d‚Äôafficher la courbe ROC")
                logger.warning(f"‚ö†Ô∏è √âchec cr√©ation courbe ROC pour {model_name}")
        else:
            st.warning("‚ö†Ô∏è Mod√®le ne supporte pas predict_proba pour la courbe ROC")
            logger.warning(f"‚ö†Ô∏è Mod√®le {model_name} sans predict_proba")

        # Courbe de Pr√©cision-Rappel
        if hasattr(model, 'predict_proba'):
            st.markdown("#### Courbe de Pr√©cision-Rappel")
            pr_plot = evaluator.create_precision_recall_curve_plot(model_result)
            if pr_plot:
                st.plotly_chart(pr_plot, width='stretch')
            else:
                st.warning("‚ö†Ô∏è Impossible d‚Äôafficher la courbe de pr√©cision-rappel")
                logger.warning(f"‚ö†Ô∏è √âchec cr√©ation courbe PR pour {model_name}")
        else:
            st.warning("‚ö†Ô∏è Mod√®le ne supporte pas predict_proba pour la courbe PR")
            logger.warning(f"‚ö†Ô∏è Mod√®le {model_name} sans predict_proba")

        # Courbe d‚Äôapprentissage
        if model_result.get('X_train') is not None and model_result.get('y_train') is not None:
            st.markdown("#### Courbe d‚ÄôApprentissage")
            learning_plot = evaluator.create_learning_curve_plot(model_result)
            if learning_plot:
                st.plotly_chart(learning_plot, width='stretch')
            else:
                st.warning("‚ö†Ô∏è Impossible d‚Äôafficher la courbe d‚Äôapprentissage")
                logger.warning(f"‚ö†Ô∏è √âchec cr√©ation courbe d‚Äôapprentissage pour {model_name}")
        else:
            st.warning("‚ö†Ô∏è Donn√©es d‚Äôentra√Ænement (X_train, y_train) manquantes pour la courbe d‚Äôapprentissage")
            logger.warning(f"‚ö†Ô∏è X_train ou y_train manquant pour {model_name}")

        # Distribution des probabilit√©s pr√©dites
        if hasattr(model, 'predict_proba'):
            st.markdown("#### Distribution des Probabilit√©s Pr√©dites (Classe Positive)")
            proba_plot = evaluator.create_predicted_proba_distribution_plot(model_result)
            if proba_plot:
                st.plotly_chart(proba_plot, width='stretch')
            else:
                st.warning("‚ö†Ô∏è Impossible d‚Äôafficher la distribution des probabilit√©s")
                logger.warning(f"‚ö†Ô∏è √âchec cr√©ation distribution probabilit√©s pour {model_name}")
        else:
            st.warning("‚ö†Ô∏è Mod√®le ne supporte pas predict_proba pour la distribution des probabilit√©s")
            logger.warning(f"‚ö†Ô∏è Mod√®le {model_name} sans predict_proba")

        # Heatmap de corr√©lation des features
        if model_result.get('X_sample') is not None and not model_result.get('X_sample').empty:
            st.markdown("#### Heatmap de Corr√©lation des Features")
            corr_plot = evaluator.create_feature_correlation_heatmap(model_result)
            if corr_plot:
                st.plotly_chart(corr_plot, width='stretch')
            else:
                st.warning("‚ö†Ô∏è Impossible d‚Äôafficher la heatmap de corr√©lation")
                logger.warning(f"‚ö†Ô∏è √âchec cr√©ation heatmap corr√©lation pour {model_name}")
        else:
            st.warning("‚ö†Ô∏è Donn√©es (X_sample) manquantes pour la heatmap de corr√©lation")
            logger.warning(f"‚ö†Ô∏è X_sample manquant pour {model_name}")

    # R√©gression : Graphique des r√©sidus, Pr√©dictions vs. R√©elles
    if task_type == 'regression' and model and model_result.get('X_test') is not None and model_result.get('y_test') is not None:
        st.markdown("#### Graphique des R√©sidus")
        residuals_plot = evaluator.create_residuals_plot(model_result)
        if residuals_plot:
            st.plotly_chart(residuals_plot, width='stretch')
        else:
            st.warning("‚ö†Ô∏è Impossible d‚Äôafficher le graphique des r√©sidus")
            logger.warning(f"‚ö†Ô∏è √âchec cr√©ation graphique r√©sidus pour {model_name}")

        st.markdown("#### Pr√©dictions vs. R√©elles")
        pred_vs_actual_plot = evaluator.create_predicted_vs_actual_plot(model_result)
        if pred_vs_actual_plot:
            st.plotly_chart(pred_vs_actual_plot, width='stretch')
        else:
            st.warning("‚ö†Ô∏è Impossible d‚Äôafficher le graphique pr√©dictions vs. r√©elles")
            logger.warning(f"‚ö†Ô∏è √âchec cr√©ation graphique pr√©dictions vs. r√©elles pour {model_name}")

    # Clustering : Scatter plot, Silhouette plot, Dispersion intra-cluster
    if task_type == 'clustering' and model_result.get('X_sample') is not None and model_result.get('labels') is not None:
        st.markdown("#### Visualisation des Clusters")
        cluster_plot = evaluator.create_cluster_scatter_plot(model_result)
        if cluster_plot:
            st.plotly_chart(cluster_plot, width='stretch')
        else:
            st.warning("‚ö†Ô∏è Impossible d‚Äôafficher le scatter plot des clusters")
            logger.warning(f"‚ö†Ô∏è √âchec cr√©ation scatter plot clusters pour {model_name}")
        
        st.markdown("#### Analyse de Silhouette")
        silhouette_plot = evaluator.create_silhouette_plot(model_result)
        if silhouette_plot:
            st.plotly_chart(silhouette_plot, width='stretch')
        else:
            st.warning("‚ö†Ô∏è Impossible d‚Äôafficher le silhouette plot")
            logger.warning(f"‚ö†Ô∏è √âchec cr√©ation silhouette plot pour {model_name}")

        st.markdown("#### Dispersion Intra-Cluster")
        intra_cluster_plot = evaluator.create_intra_cluster_distance_plot(model_result)
        if intra_cluster_plot:
            st.plotly_chart(intra_cluster_plot, width='stretch')
        else:
            st.warning("‚ö†Ô∏è Impossible d‚Äôafficher le graphique de dispersion intra-cluster")
            logger.warning(f"‚ö†Ô∏è √âchec cr√©ation dispersion intra-cluster pour {model_name}")

def main():
    if 'ml_results' not in st.session_state or not st.session_state.ml_results:
        st.error("üö´ Aucun r√©sultat disponible")
        st.info("Entra√Ænez des mod√®les dans 'Configuration ML'.")
        if st.button("‚öôÔ∏è Aller √† Configuration ML", width='stretch'):
            st.switch_page("pages/2_‚öôÔ∏è_Configuration_ML.py")
        return

    try:
        evaluator = ModelEvaluationVisualizer(st.session_state.ml_results)
        validation = evaluator.validation_result
    except Exception as e:
        st.error(f"‚ùå Erreur initialisation: {str(e)}")
        st.info("Action: V√©rifiez les r√©sultats ou relancez l'entra√Ænement.")
        return

    if not validation["has_results"]:
        st.error("üì≠ Aucune donn√©e valide")
        return

    display_metrics_header(validation)

    tab1, tab2, tab3, tab4 = st.tabs(["üìä Vue d'Ensemble", "üîç D√©tails", "üìà M√©triques", "üíæ Export"])
    
    with tab1:
        st.markdown('<div class="tab-content">', unsafe_allow_html=True)
        if validation["successful_models"]:
            st.markdown("### üìà Comparaison des Performances")
            comparison_plot = evaluator.get_comparison_plot()
            if comparison_plot:
                st.plotly_chart(cached_plot(comparison_plot, "comparison_plot"), width='stretch')
            
            st.markdown("### üìã Synth√®se")
            df_comparison = evaluator.get_comparison_dataframe()
            st.dataframe(df_comparison, width='stretch', height=400)
            
            st.markdown('<div class="section-divider"></div>', unsafe_allow_html=True)
            st.markdown("### üìä R√©sum√© Statistique")
            col1, col2, col3 = st.columns(3)
            if validation["task_type"] in ['classification', 'regression']:
                numeric_cols = df_comparison.select_dtypes(include=[np.number])
                if not numeric_cols.empty:
                    main_metric = 'Accuracy' if validation["task_type"] == 'classification' else 'R¬≤'
                    if main_metric in numeric_cols.columns:
                        with col1:
                            st.metric("Score Moyen", f"{numeric_cols[main_metric].mean():.3f}")
                        with col2:
                            st.metric("Meilleur Score", f"{numeric_cols[main_metric].max():.3f}")
                        with col3:
                            st.metric("√âcart-type", f"{numeric_cols[main_metric].std():.3f}")
            elif validation["task_type"] == 'clustering':
                with col1:
                    avg_silhouette = np.mean([r.get('metrics', {}).get('silhouette_score', 0) 
                                            for r in validation["successful_models"]])
                    st.metric("Silhouette Moyen", f"{avg_silhouette:.3f}")
                with col2:
                    avg_clusters = np.mean([r.get('metrics', {}).get('n_clusters', 0) 
                                          for r in validation["successful_models"]])
                    st.metric("Clusters Moyen", f"{avg_clusters:.1f}")
                with col3:
                    best_silhouette = max([r.get('metrics', {}).get('silhouette_score', 0) 
                                         for r in validation["successful_models"]])
                    st.metric("Meilleur Silhouette", f"{best_silhouette:.3f}")
        else:
            st.warning("‚ö†Ô∏è Aucun mod√®le valide")
        st.markdown('</div>', unsafe_allow_html=True)

    with tab2:
        st.markdown('<div class="tab-content">', unsafe_allow_html=True)
        st.markdown("### üîç Analyse D√©taill√©e")
        if validation["successful_models"]:
            model_names = [r.get('model_name', f'Mod√®le_{i}') 
                         for i, r in enumerate(validation["successful_models"])]
            selected_idx = st.selectbox(
                "Mod√®le √† analyser:",
                range(len(model_names)),
                format_func=lambda x: f"{model_names[x]} {'üèÜ' if model_names[x]==validation.get('best_model') else ''}",
                key="model_selector_detail"
            )
            model_result = validation["successful_models"][selected_idx]
            display_model_details(evaluator, model_result, validation["task_type"])
        else:
            st.info("‚ÑπÔ∏è Aucun mod√®le disponible")
        st.markdown('</div>', unsafe_allow_html=True)

    with tab3:
        st.markdown('<div class="tab-content">', unsafe_allow_html=True)
        st.markdown("### üìà M√©triques Avanc√©es")
        if validation["successful_models"]:
            st.markdown("#### üìä Distribution des Performances")
            dist_plot = evaluator.get_performance_distribution_plot()
            if dist_plot:
                st.plotly_chart(cached_plot(dist_plot, "dist_plot"), width='stretch')
            
            st.markdown("#### üìã M√©triques par Cat√©gorie")
            if validation["task_type"] == 'clustering':
                st.markdown("**üéØ Qualit√© des Clusters**")
                clustering_metrics = []
                for result in validation["successful_models"]:
                    metrics = result.get('metrics', {})
                    silhouette = metrics.get('silhouette_score', 0)
                    clustering_metrics.append({
                        'Mod√®le': result.get('model_name', 'Unknown'),
                        'Silhouette': f"{silhouette:.3f}",
                        'Calinski-Harabasz': f"{metrics.get('calinski_harabasz', 0):.3f}",
                        'Davies-Bouldin': f"{metrics.get('davies_bouldin_score', 0):.3f}",
                        'Clusters': metrics.get('n_clusters', 'N/A'),
                        'Qualit√©': 'üü¢ Excellente' if silhouette > 0.7 else 'üü° Bonne' if silhouette > 0.5 else 'üü† Moyenne' if silhouette > 0.3 else 'üî¥ Faible'
                    })
                st.dataframe(pd.DataFrame(clustering_metrics), width='stretch')
                
                st.markdown("**‚è±Ô∏è Performance Computationnelle**")
                perf_metrics = [
                    {'Mod√®le': r.get('model_name'), 'Temps (s)': f"{r.get('training_time', 0):.1f}"}
                    for r in validation["successful_models"]
                ]
                st.dataframe(pd.DataFrame(perf_metrics), width='stretch')
            
            elif validation["task_type"] == 'classification':
                st.markdown("**üè∑Ô∏è Pr√©cision**")
                class_metrics = [
                    {
                        'Mod√®le': r.get('model_name', 'Unknown'),
                        'Accuracy': f"{r.get('metrics', {}).get('accuracy', 0):.3f}",
                        'Precision': f"{r.get('metrics', {}).get('precision', 0):.3f}",
                        'Recall': f"{r.get('metrics', {}).get('recall', 0):.3f}",
                        'F1-Score': f"{r.get('metrics', {}).get('f1_score', 0):.3f}"
                    }
                    for r in validation["successful_models"]
                ]
                st.dataframe(pd.DataFrame(class_metrics), width='stretch')
                
                st.markdown("**‚è±Ô∏è Performance Computationnelle**")
                perf_metrics = [
                    {'Mod√®le': r.get('model_name'), 'Temps (s)': f"{r.get('training_time', 0):.1f}"}
                    for r in validation["successful_models"]
                ]
                st.dataframe(pd.DataFrame(perf_metrics), width='stretch')
            
            elif validation["task_type"] == 'regression':
                st.markdown("**üìä Pr√©cision**")
                reg_metrics = [
                    {
                        'Mod√®le': r.get('model_name', 'Unknown'),
                        'R¬≤ Score': f"{r.get('metrics', {}).get('r2', 0):.3f}",
                        'MAE': f"{r.get('metrics', {}).get('mae', 0):.3f}",
                        'RMSE': f"{r.get('metrics', {}).get('rmse', 0):.3f}"
                    }
                    for r in validation["successful_models"]
                ]
                st.dataframe(pd.DataFrame(reg_metrics), width='stretch')
                
                st.markdown("**‚è±Ô∏è Performance Computationnelle**")
                perf_metrics = [
                    {'Mod√®le': r.get('model_name'), 'Temps (s)': f"{r.get('training_time', 0):.1f}"}
                    for r in validation["successful_models"]
                ]
                st.dataframe(pd.DataFrame(perf_metrics), width='stretch')
        else:
            st.warning("‚ö†Ô∏è Aucune m√©trique disponible")
        st.markdown('</div>', unsafe_allow_html=True)

    with tab4:
        st.markdown('<div class="tab-content">', unsafe_allow_html=True)
        st.markdown("### üíæ Export des R√©sultats")
        if validation["successful_models"]:
            export_data = evaluator.get_export_data()
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("#### üìä Donn√©es Structur√©es")
                csv_data = pd.DataFrame(export_data['models']).to_csv(index=False)
                st.download_button(
                    label="üì• T√©l√©charger CSV",
                    data=csv_data,
                    file_name=f"evaluation_{validation['task_type']}_{int(time.time())}.csv",
                    mime="text/csv",
                    width='stretch'
                )
                st.download_button(
                    label="üì• T√©l√©charger JSON",
                    data=json.dumps(export_data, indent=2, ensure_ascii=False, default=str),
                    file_name=f"evaluation_{validation['task_type']}_{int(time.time())}.json",
                    mime="application/json",
                    width='stretch'
                )
                with st.expander("üëÅÔ∏è Aper√ßu"):
                    st.json(export_data, expanded=False)
            with col2:
                st.markdown("#### üìà Rapport Global")
                if validation["best_model"]:
                    best_model_result = next((r for r in validation["successful_models"] 
                                           if r.get('model_name') == validation["best_model"]), None)
                    if best_model_result:
                        pdf_bytes = create_pdf_report_latex(best_model_result, validation["task_type"])
                        if pdf_bytes:
                            st.download_button(
                                label="üìÑ Rapport PDF",
                                data=pdf_bytes,
                                file_name=f"rapport_{validation['best_model']}_{int(time.time())}.pdf",
                                mime="application/pdf",
                                width='stretch'
                            )
                if st.button("üîÑ G√©n√©rer Rapport", width='stretch'):
                    with st.spinner("G√©n√©ration..."):
                        time.sleep(1)
                        st.success("‚úÖ Rapport g√©n√©r√©!")
                        st.balloons()
        else:
            st.info("‚ÑπÔ∏è Aucune donn√©e disponible")
        st.markdown('</div>', unsafe_allow_html=True)

    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.caption(f"üïê Mis √† jour: {time.strftime('%H:%M:%S')}")
    with col2:
        memory_info = get_system_metrics()
        memory_status = "üü¢" if memory_info['memory_percent'] < 70 else "üü°" if memory_info['memory_percent'] < 85 else "üî¥"
        st.caption(f"{memory_status} M√©moire: {memory_info['memory_percent']:.1f}%")
    with col3:
        st.caption(f"üìä Mod√®les: {len(st.session_state.ml_results)}")

if __name__ == "__main__":
    main()