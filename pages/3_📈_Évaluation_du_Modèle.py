import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import joblib
import json
import time
import logging
import psutil
import gc
from typing import Dict, List, Any, Optional, Tuple
import base64
from io import BytesIO
import os

# Configuration
logger = logging.getLogger(__name__)
st.set_page_config(
    page_title="√âvaluation des Mod√®les", 
    page_icon="üìà", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Configuration Production ---
def setup_evaluation_environment():
    """Configuration pour l'environnement de production"""
    if 'evaluation_setup_done' not in st.session_state:
        st.session_state.evaluation_setup_done = True
        
        # Masquer les √©l√©ments Streamlit en production
        if os.getenv('STREAMLIT_ENV') == 'production':
            hide_streamlit_style = """
            <style>
            #MainMenu {visibility: hidden;}
            .stDeployButton {display:none;}
            footer {visibility: hidden;}
            #stDecoration {display:none;}
            </style>
            """
            st.markdown(hide_streamlit_style, unsafe_allow_html=True)

setup_evaluation_environment()

# --- Styles CSS personnalis√©s ---
st.markdown("""
<style>
    .metric-card {
        background-color: #f0f2f6;
        padding: 15px;
        border-radius: 10px;
        border-left: 4px solid #ff4b4b;
        margin: 10px 0px;
    }
    .best-model {
        background-color: #e6f7ff;
        border-left: 4px solid #1890ff;
    }
    .model-comparison {
        background-color: #f6f6f6;
        border-radius: 10px;
        padding: 15px;
        margin: 10px 0px;
    }
    .feature-importance {
        background-color: #fff7e6;
        border-left: 4px solid #fa8c16;
    }
</style>
""", unsafe_allow_html=True)

# --- Fonctions utilitaires ---
def get_system_metrics() -> Dict[str, Any]:
    """R√©cup√®re les m√©triques syst√®me"""
    try:
        memory = psutil.virtual_memory()
        return {
            'memory_percent': memory.percent,
            'memory_available_mb': memory.available / (1024 * 1024),
            'timestamp': time.time()
        }
    except Exception as e:
        logger.error(f"Failed to get system metrics: {e}")
        return {'memory_percent': 0, 'memory_available_mb': 0, 'timestamp': time.time()}

def safe_get(obj, keys, default=None):
    """Acc√®s s√©curis√© aux donn√©es nested"""
    try:
        for key in keys:
            obj = obj[key]
        return obj
    except (KeyError, TypeError, IndexError):
        return default

def format_metric_value(value, metric_name: str) -> str:
    """Formate les valeurs m√©triques selon leur type"""
    if value is None:
        return "N/A"
    
    if isinstance(value, (int, np.integer)):
        return f"{value:,}"
    
    if isinstance(value, float):
        if metric_name in ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'r2']:
            return f"{value:.3f}"
        elif metric_name in ['mse', 'mae', 'rmse']:
            return f"{value:.4f}"
        else:
            return f"{value:.3f}"
    
    return str(value)

def create_download_link(data, filename: str, file_type: str = "csv") -> str:
    """Cr√©e un lien de t√©l√©chargement"""
    try:
        if file_type == "csv":
            csv = data.to_csv(index=False)
            b64 = base64.b64encode(csv.encode()).decode()
            return f'<a href="data:file/csv;base64,{b64}" download="{filename}.csv">üì• T√©l√©charger CSV</a>'
        elif file_type == "json":
            json_str = json.dumps(data, indent=2, ensure_ascii=False)
            b64 = base64.b64encode(json_str.encode()).decode()
            return f'<a href="data:application/json;base64,{b64}" download="{filename}.json">üì• T√©l√©charger JSON</a>'
    except Exception as e:
        logger.error(f"Erreur cr√©ation lien t√©l√©chargement: {e}")
    return ""

# --- V√©rification des donn√©es ---
def validate_evaluation_data() -> Dict[str, Any]:
    """Valide la pr√©sence des donn√©es d'√©valuation"""
    validation = {
        "has_results": False,
        "results_count": 0,
        "task_type": None,
        "best_model": None,
        "errors": []
    }
    
    try:
        if 'ml_results' not in st.session_state or not st.session_state.ml_results:
            validation["errors"].append("Aucun r√©sultat d'exp√©rimentation trouv√©")
            return validation
        
        results = st.session_state.ml_results
        validation["results_count"] = len(results)
        validation["has_results"] = True
        
        # D√©terminer le type de t√¢che
        if results and 'task_type' in results[0]:
            validation["task_type"] = results[0]['task_type']
        else:
            # D√©duire du premier mod√®le qui a des m√©triques valides
            for result in results:
                metrics = result.get('metrics', {})
                if 'accuracy' in metrics:
                    validation["task_type"] = 'classification'
                    break
                elif 'r2' in metrics:
                    validation["task_type"] = 'regression'
                    break
                elif 'silhouette_score' in metrics:
                    validation["task_type"] = 'unsupervised'
                    break
        
        # Trouver le meilleur mod√®le
        successful_models = [r for r in results if not r.get('metrics', {}).get('error')]
        if successful_models:
            if validation["task_type"] == 'classification':
                successful_models.sort(key=lambda x: x.get('metrics', {}).get('accuracy', 0), reverse=True)
            elif validation["task_type"] == 'regression':
                successful_models.sort(key=lambda x: x.get('metrics', {}).get('r2', -float('inf')), reverse=True)
            elif validation["task_type"] == 'unsupervised':
                successful_models.sort(key=lambda x: x.get('metrics', {}).get('silhouette_score', -float('inf')), reverse=True)
            
            if successful_models:
                validation["best_model"] = successful_models[0]['model_name']
        
    except Exception as e:
        validation["errors"].append(f"Erreur validation donn√©es: {str(e)}")
        logger.error(f"Erreur validation √©valuation: {e}")
    
    return validation

# --- Visualisations ---
def create_metrics_comparison_plot(results: List[Dict], task_type: str) -> go.Figure:
    """Cr√©e un graphique de comparaison des m√©triques"""
    try:
        # Filtrer les mod√®les avec des m√©triques valides
        valid_results = [r for r in results if not r.get('metrics', {}).get('error')]
        
        if not valid_results:
            fig = go.Figure()
            fig.add_annotation(text="Aucune m√©trique valide disponible", x=0.5, y=0.5, showarrow=False)
            return fig
        
        model_names = [r['model_name'] for r in valid_results]
        
        if task_type == 'classification':
            # M√©triques de classification
            accuracies = [safe_get(r, ['metrics', 'accuracy'], 0) for r in valid_results]
            f1_scores = [safe_get(r, ['metrics', 'f1_score'], 0) for r in valid_results]
            roc_aucs = [safe_get(r, ['metrics', 'roc_auc'], 0) for r in valid_results]
            
            fig = make_subplots(rows=1, cols=3, subplot_titles=('Accuracy', 'F1-Score', 'AUC-ROC'))
            
            fig.add_trace(go.Bar(x=model_names, y=accuracies, name='Accuracy'), 1, 1)
            fig.add_trace(go.Bar(x=model_names, y=f1_scores, name='F1-Score'), 1, 2)
            fig.add_trace(go.Bar(x=model_names, y=roc_aucs, name='AUC-ROC'), 1, 3)
            
        elif task_type == 'regression':
            # M√©triques de r√©gression
            r2_scores = [safe_get(r, ['metrics', 'r2'], 0) for r in valid_results]
            mae_scores = [safe_get(r, ['metrics', 'mae'], 0) for r in valid_results]
            rmse_scores = [safe_get(r, ['metrics', 'rmse'], 0) for r in valid_results]
            
            fig = make_subplots(rows=1, cols=3, subplot_titles=('R¬≤ Score', 'MAE', 'RMSE'))
            
            fig.add_trace(go.Bar(x=model_names, y=r2_scores, name='R¬≤'), 1, 1)
            fig.add_trace(go.Bar(x=model_names, y=mae_scores, name='MAE'), 1, 2)
            fig.add_trace(go.Bar(x=model_names, y=rmse_scores, name='RMSE'), 1, 3)
            
        else:
            # M√©triques non supervis√©es
            silhouette_scores = [safe_get(r, ['metrics', 'silhouette_score'], 0) for r in valid_results]
            n_clusters = [safe_get(r, ['metrics', 'n_clusters'], 0) for r in valid_results]
            
            fig = make_subplots(rows=1, cols=2, subplot_titles=('Silhouette Score', 'Nombre de Clusters'))
            
            fig.add_trace(go.Bar(x=model_names, y=silhouette_scores, name='Silhouette'), 1, 1)
            fig.add_trace(go.Bar(x=model_names, y=n_clusters, name='Clusters'), 1, 2)
        
        fig.update_layout(
            title=f"Comparaison des Mod√®les - {task_type.upper()}",
            height=400,
            showlegend=False
        )
        
        return fig
        
    except Exception as e:
        logger.error(f"Erreur cr√©ation graphique comparaison: {e}")
        fig = go.Figure()
        fig.add_annotation(text="Erreur lors de la cr√©ation du graphique", x=0.5, y=0.5, showarrow=False)
        return fig

def create_confusion_matrix_plot(confusion_matrix: List[List[int]], class_names: List[str] = None) -> go.Figure:
    """Cr√©e une heatmap de matrice de confusion"""
    try:
        if confusion_matrix is None:
            fig = go.Figure()
            fig.add_annotation(text="Matrice de confusion non disponible", x=0.5, y=0.5, showarrow=False)
            return fig
        
        cm_array = np.array(confusion_matrix)
        
        if class_names is None:
            class_names = [f"Classe {i}" for i in range(len(cm_array))]
        
        fig = px.imshow(
            cm_array,
            labels=dict(x="Pr√©dit", y="R√©el", color="Count"),
            x=class_names,
            y=class_names,
            color_continuous_scale='Blues',
            aspect="auto"
        )
        
        # Ajouter les annotations
        for i in range(len(cm_array)):
            for j in range(len(cm_array[i])):
                fig.add_annotation(
                    x=j, y=i,
                    text=str(cm_array[i, j]),
                    showarrow=False,
                    font=dict(color="red" if cm_array[i, j] > cm_array.max() / 2 else "black")
                )
        
        fig.update_layout(
            title="Matrice de Confusion",
            xaxis_title="Pr√©dit",
            yaxis_title="R√©el"
        )
        
        return fig
        
    except Exception as e:
        logger.error(f"Erreur cr√©ation matrice confusion: {e}")
        fig = go.Figure()
        fig.add_annotation(text="Erreur cr√©ation matrice de confusion", x=0.5, y=0.5, showarrow=False)
        return fig

def create_feature_importance_plot(model, feature_names: List[str]) -> go.Figure:
    """Cr√©e un graphique d'importance des features"""
    try:
        # V√©rifier si le mod√®le a une importance des features
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
        elif hasattr(model, 'coef_'):
            importances = np.abs(model.coef_.flatten())
        else:
            fig = go.Figure()
            fig.add_annotation(text="Importance des features non disponible", x=0.5, y=0.5, showarrow=False)
            return fig
        
        # Cr√©er le graphique
        indices = np.argsort(importances)[::-1]
        sorted_importances = importances[indices]
        sorted_features = [feature_names[i] for i in indices]
        
        fig = go.Figure(go.Bar(
            x=sorted_importances[:10],  # Top 10 features
            y=sorted_features[:10],
            orientation='h'
        ))
        
        fig.update_layout(
            title="Top 10 - Importance des Features",
            xaxis_title="Importance",
            yaxis_title="Features",
            height=400
        )
        
        return fig
        
    except Exception as e:
        logger.error(f"Erreur cr√©ation importance features: {e}")
        fig = go.Figure()
        fig.add_annotation(text="Erreur cr√©ation importance features", x=0.5, y=0.5, showarrow=False)
        return fig

# --- Interface principale ---
st.title("üìà √âvaluation des Mod√®les")

# V√©rification des donn√©es
validation = validate_evaluation_data()

if not validation["has_results"]:
    st.error("‚ùå Aucun r√©sultat d'exp√©rimentation trouv√©")
    st.info("""
    **Pour utiliser cette page :**
    1. Allez sur la page **‚öôÔ∏è Configuration ML**
    2. Configurez et lancez une exp√©rimentation
    3. Revenez sur cette page pour voir les r√©sultats
    """)
    st.page_link("pages/2_‚öôÔ∏è_Configuration_ML.py", label="üîß Aller √† la configuration ML", icon="‚öôÔ∏è")
    st.stop()

# M√©triques syst√®me
system_metrics = get_system_metrics()
with st.sidebar:
    st.subheader("üìä Statistiques")
    st.write(f"**Mod√®les √©valu√©s :** {validation['results_count']}")
    if validation['best_model']:
        st.write(f"**Meilleur mod√®le :** {validation['best_model']}")
    st.write(f"**Type de t√¢che :** {validation['task_type']}")
    
    st.subheader("üîß Actions")
    if st.button("üîÑ Actualiser les donn√©es"):
        st.rerun()
    
    if st.button("üßπ Nettoyer la m√©moire"):
        gc.collect()
        st.success("M√©moire nettoy√©e!")
    
    st.subheader("üíæ Export")
    if st.button("üìä Exporter les r√©sultats"):
        # Cr√©er un rapport exportable
        export_data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "task_type": validation["task_type"],
            "models_count": validation["results_count"],
            "best_model": validation["best_model"],
            "results": st.session_state.ml_results
        }
        
        # Convertir en JSON t√©l√©chargeable
        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)
        st.download_button(
            label="üì• T√©l√©charger le rapport JSON",
            data=json_str,
            file_name=f"rapport_ml_{time.strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json"
        )

# Header avec r√©sum√©
st.success(f"‚úÖ Exp√©rimentation termin√©e - {validation['results_count']} mod√®les √©valu√©s")

# M√©triques rapides
successful_models = [r for r in st.session_state.ml_results if not r.get('metrics', {}).get('error')]
failed_models = [r for r in st.session_state.ml_results if r.get('metrics', {}).get('error')]

col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("Mod√®les r√©ussis", len(successful_models))
with col2:
    st.metric("Mod√®les √©chou√©s", len(failed_models))
with col3:
    if validation['task_type'] == 'classification':
        best_score = safe_get(successful_models[0] if successful_models else {}, ['metrics', 'accuracy'], 0)
        st.metric("Meilleure accuracy", f"{best_score:.3f}")
    elif validation['task_type'] == 'regression':
        best_score = safe_get(successful_models[0] if successful_models else {}, ['metrics', 'r2'], 0)
        st.metric("Meilleur R¬≤", f"{best_score:.3f}")
with col4:
    st.metric("RAM utilis√©e", f"{system_metrics['memory_percent']:.1f}%")

# --- Onglets principaux ---
tab_comparison, tab_individual, tab_details, tab_export = st.tabs([
    "üìä Comparaison Globale", 
    "üîç Analyse Individuelle", 
    "üìã D√©tails Techniques", 
    "üíæ Export & Rapport"
])

# Onglet 1: Comparaison Globale
with tab_comparison:
    st.header("üìä Comparaison des Performances")
    
    # Graphique de comparaison
    fig = create_metrics_comparison_plot(st.session_state.ml_results, validation["task_type"])
    st.plotly_chart(fig, use_container_width=True)
    
    # Tableau de comparaison d√©taill√©
    st.subheader("üìã Tableau Comparatif")
    
    comparison_data = []
    for result in st.session_state.ml_results:
        model_info = {
            'Mod√®le': result['model_name'],
            'Statut': '‚úÖ Succ√®s' if not result.get('metrics', {}).get('error') else '‚ùå √âchec',
            'Temps (s)': safe_get(result, ['training_time'], 'N/A')
        }
        
        metrics = result.get('metrics', {})
        if validation["task_type"] == 'classification':
            model_info.update({
                'Accuracy': safe_get(metrics, ['accuracy']),
                'F1-Score': safe_get(metrics, ['f1_score']),
                'AUC-ROC': safe_get(metrics, ['roc_auc']),
                'Pr√©cision': safe_get(metrics, ['precision']),
                'Rappel': safe_get(metrics, ['recall'])
            })
        elif validation["task_type"] == 'regression':
            model_info.update({
                'R¬≤': safe_get(metrics, ['r2']),
                'MAE': safe_get(metrics, ['mae']),
                'RMSE': safe_get(metrics, ['rmse']),
                'MSE': safe_get(metrics, ['mse'])
            })
        elif validation["task_type"] == 'unsupervised':
            model_info.update({
                'Silhouette': safe_get(metrics, ['silhouette_score']),
                'Clusters': safe_get(metrics, ['n_clusters']),
                'Davies-Bouldin': safe_get(metrics, ['davies_bouldin_score'])
            })
        
        comparison_data.append(model_info)
    
    df_comparison = pd.DataFrame(comparison_data)
    st.dataframe(df_comparison, use_container_width=True, height=400)
    
    # Bouton d'export
    st.markdown(create_download_link(df_comparison, "comparaison_modeles", "csv"), unsafe_allow_html=True)

# Onglet 2: Analyse Individuelle
with tab_individual:
    st.header("üîç Analyse par Mod√®le")
    
    # S√©lecteur de mod√®le
    model_names = [r['model_name'] for r in st.session_state.ml_results]
    selected_model = st.selectbox("S√©lectionnez un mod√®le √† analyser", model_names)
    
    # R√©cup√©rer les donn√©es du mod√®le s√©lectionn√©
    selected_result = next((r for r in st.session_state.ml_results if r['model_name'] == selected_model), None)
    
    if selected_result:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader(f"Performances de {selected_model}")
            
            metrics = selected_result.get('metrics', {})
            if metrics.get('error'):
                st.error(f"‚ùå Erreur lors de l'entra√Ænement : {metrics['error']}")
            else:
                # Affichage des m√©triques principales
                if validation["task_type"] == 'classification':
                    cols = st.columns(4)
                    with cols[0]:
                        st.metric("Accuracy", format_metric_value(metrics.get('accuracy'), 'accuracy'))
                    with cols[1]:
                        st.metric("F1-Score", format_metric_value(metrics.get('f1_score'), 'f1_score'))
                    with cols[2]:
                        st.metric("AUC-ROC", format_metric_value(metrics.get('roc_auc'), 'roc_auc'))
                    with cols[3]:
                        st.metric("Pr√©cision", format_metric_value(metrics.get('precision'), 'precision'))
                    
                    # Matrice de confusion si disponible
                    if 'confusion_matrix' in metrics:
                        st.subheader("üéØ Matrice de Confusion")
                        cm_fig = create_confusion_matrix_plot(metrics['confusion_matrix'])
                        st.plotly_chart(cm_fig, use_container_width=True)
                
                elif validation["task_type"] == 'regression':
                    cols = st.columns(4)
                    with cols[0]:
                        st.metric("R¬≤ Score", format_metric_value(metrics.get('r2'), 'r2'))
                    with cols[1]:
                        st.metric("MAE", format_metric_value(metrics.get('mae'), 'mae'))
                    with cols[2]:
                        st.metric("RMSE", format_metric_value(metrics.get('rmse'), 'rmse'))
                    with cols[3]:
                        st.metric("MSE", format_metric_value(metrics.get('mse'), 'mse'))
                
                elif validation["task_type"] == 'unsupervised':
                    cols = st.columns(3)
                    with cols[0]:
                        st.metric("Score Silhouette", format_metric_value(metrics.get('silhouette_score'), 'silhouette_score'))
                    with cols[1]:
                        st.metric("Nombre de Clusters", metrics.get('n_clusters', 'N/A'))
                    with cols[2]:
                        st.metric("Davies-Bouldin", format_metric_value(metrics.get('davies_bouldin_score'), 'davies_bouldin_score'))
        
        with col2:
            st.subheader("üìä Informations")
            
            # Carte d'information du mod√®le
            st.markdown(f"""
            <div class="metric-card">
                <h4>üìã Informations Mod√®le</h4>
                <p><strong>Statut:</strong> {'‚úÖ Entra√Æn√©' if not metrics.get('error') else '‚ùå Erreur'}</p>
                <p><strong>Temps d'entra√Ænement:</strong> {safe_get(selected_result, ['training_time'], 'N/A')}s</p>
                <p><strong>Type de t√¢che:</strong> {validation['task_type']}</p>
            </div>
            """, unsafe_allow_html=True)
            
            # Importance des features si disponible
            if 'model' in selected_result and selected_result['model'] is not None:
                try:
                    feature_names = selected_result.get('feature_names', [])
                    if feature_names:
                        importance_fig = create_feature_importance_plot(selected_result['model'], feature_names)
                        st.plotly_chart(importance_fig, use_container_width=True)
                except Exception as e:
                    logger.debug(f"Importance features non disponible: {e}")

# Onglet 3: D√©tails Techniques
with tab_details:
    st.header("üìã D√©tails Techniques")
    
    # S√©lecteur de mod√®le pour les d√©tails
    model_for_details = st.selectbox("Mod√®le pour d√©tails techniques", model_names, key="details_selector")
    detailed_result = next((r for r in st.session_state.ml_results if r['model_name'] == model_for_details), None)
    
    if detailed_result:
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("‚öôÔ∏è Configuration")
            
            # Hyperparam√®tres
            st.write("**Hyperparam√®tres optimis√©s :**")
            best_params = detailed_result.get('best_params', {})
            if best_params:
                for param, value in best_params.items():
                    st.write(f"- `{param}`: `{value}`")
            else:
                st.info("Aucun hyperparam√®tre optimis√© (entra√Ænement standard)")
            
            # Features utilis√©es
            feature_list = detailed_result.get('feature_names', [])
            if feature_list:
                st.write(f"**Features utilis√©es ({len(feature_list)}) :**")
                features_text = ", ".join([f"`{f}`" for f in feature_list[:10]])
                if len(feature_list) > 10:
                    features_text += f" ... et {len(feature_list) - 10} de plus"
                st.write(features_text)
        
        with col2:
            st.subheader("üìà M√©triques D√©taill√©es")
            
            metrics = detailed_result.get('metrics', {})
            if not metrics.get('error'):
                # Affichage format√© des m√©triques
                metric_df_data = []
                for metric_name, metric_value in metrics.items():
                    if metric_name not in ['confusion_matrix', 'classification_report', 'error_stats']:
                        metric_df_data.append({
                            'M√©trique': metric_name.replace('_', ' ').title(),
                            'Valeur': format_metric_value(metric_value, metric_name)
                        })
                
                if metric_df_data:
                    metric_df = pd.DataFrame(metric_df_data)
                    st.dataframe(metric_df, use_container_width=True, height=300)
                
                # Rapport de classification d√©taill√©
                if 'classification_report' in metrics and validation["task_type"] == 'classification':
                    with st.expander("üìä Rapport de Classification D√©taill√©"):
                        try:
                            report_df = pd.DataFrame(metrics['classification_report']).transpose()
                            st.dataframe(report_df, use_container_width=True)
                        except Exception as e:
                            st.error(f"Erreur affichage rapport: {e}")

# Onglet 4: Export & Rapport
with tab_export:
    st.header("üíæ Export des R√©sultats")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìä Export des Donn√©es")
        
        # Format d'export
        export_format = st.radio("Format d'export", ["CSV", "JSON", "Les deux"])
        
        # Options d'export
        include_metrics = st.checkbox("Inclure les m√©triques d√©taill√©es", value=True)
        include_config = st.checkbox("Inclure la configuration", value=True)
        include_models = st.checkbox("Inclure les mod√®les (fichiers .joblib)", value=False)
        
        # Bouton d'export
        if st.button("üöÄ G√©n√©rer l'export complet", type="primary"):
            with st.spinner("G√©n√©ration de l'export en cours..."):
                try:
                    # Pr√©paration des donn√©es d'export
                    export_data = {
                        "metadata": {
                            "export_date": time.strftime("%Y-%m-%d %H:%M:%S"),
                            "task_type": validation["task_type"],
                            "models_count": len(st.session_state.ml_results),
                            "best_model": validation["best_model"]
                        },
                        "results": []
                    }
                    
                    # Ajout des r√©sultats d√©taill√©s
                    for result in st.session_state.ml_results:
                        result_export = {
                            "model_name": result['model_name'],
                            "training_time": result.get('training_time'),
                            "status": "success" if not result.get('metrics', {}).get('error') else "error"
                        }
                        
                        if include_metrics:
                            result_export["metrics"] = result.get('metrics', {})
                        
                        if include_config:
                            result_export["best_params"] = result.get('best_params', {})
                            result_export["feature_names"] = result.get('feature_names', [])
                        
                        export_data["results"].append(result_export)
                    
                    # G√©n√©ration des fichiers
                    if export_format in ["JSON", "Les deux"]:
                        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)
                        st.download_button(
                            label="üì• T√©l√©charger JSON",
                            data=json_str,
                            file_name=f"rapport_ml_complet_{time.strftime('%Y%m%d_%H%M%S')}.json",
                            mime="application/json"
                        )
                    
                    if export_format in ["CSV", "Les deux"]:
                        # Cr√©ation d'un CSV simplifi√©
                        csv_data = []
                        for result in st.session_state.ml_results:
                            row = {
                                "model_name": result['model_name'],
                                "training_time": result.get('training_time'),
                                "status": "success" if not result.get('metrics', {}).get('error') else "error"
                            }
                            
                            # Ajout des m√©triques principales
                            metrics = result.get('metrics', {})
                            for metric_name in ['accuracy', 'r2', 'silhouette_score', 'f1_score', 'mae', 'rmse']:
                                if metric_name in metrics:
                                    row[metric_name] = metrics[metric_name]
                            
                            csv_data.append(row)
                        
                        df_export = pd.DataFrame(csv_data)
                        csv_export = df_export.to_csv(index=False)
                        st.download_button(
                            label="üì• T√©l√©charger CSV",
                            data=csv_export,
                            file_name=f"rapport_ml_simplifie_{time.strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv"
                        )
                    
                    st.success("‚úÖ Export g√©n√©r√© avec succ√®s!")
                    
                except Exception as e:
                    st.error(f"‚ùå Erreur lors de l'export: {e}")
    
    with col2:
        st.subheader("üìã Rapport Automatique")
        
        # G√©n√©ration de rapport synth√©tique
        if st.button("üìÑ G√©n√©rer le rapport synth√©tique"):
            try:
                # Analyse des r√©sultats
                successful_count = len(successful_models)
                failed_count = len(failed_models)
                
                # Meilleur mod√®le
                best_model_info = successful_models[0] if successful_models else None
                
                # Cr√©ation du rapport
                report = f"""
# üìä RAPPORT D'EXP√âRIMENTATION ML

## üìã R√©sum√© Ex√©cutif
- **Date de g√©n√©ration**: {time.strftime("%Y-%m-%d %H:%M:%S")}
- **Type de t√¢che**: {validation['task_type'].upper()}
- **Mod√®les √©valu√©s**: {len(st.session_state.ml_results)}
- **Taux de succ√®s**: {successful_count}/{len(st.session_state.ml_results)} ({(successful_count/len(st.session_state.ml_results)*100):.1f}%)

## üèÜ Meilleur Mod√®le
- **Nom**: {best_model_info['model_name'] if best_model_info else 'Aucun'}
- **Score principal**: {safe_get(best_model_info, ['metrics', 'accuracy' if validation['task_type'] == 'classification' else 'r2'], 'N/A')}

## üìà Recommandations
"""
                
                if validation['task_type'] == 'classification':
                    best_accuracy = safe_get(best_model_info, ['metrics', 'accuracy'], 0)
                    if best_accuracy > 0.9:
                        report += "- ‚úÖ Performance excellente, mod√®le pr√™t pour la production\n"
                    elif best_accuracy > 0.7:
                        report += "- ‚ö†Ô∏è Performance acceptable, envisager l'optimisation\n"
                    else:
                        report += "- ‚ùå Performance faible, revoir l'approche\n"
                
                elif validation['task_type'] == 'regression':
                    best_r2 = safe_get(best_model_info, ['metrics', 'r2'], 0)
                    if best_r2 > 0.8:
                        report += "- ‚úÖ Tr√®s bon pouvoir pr√©dictif\n"
                    elif best_r2 > 0.5:
                        report += "- ‚ö†Ô∏è Pouvoir pr√©dictif acceptable\n"
                    else:
                        report += "- ‚ùå Faible pouvoir pr√©dictif\n"
                
                st.text_area("Rapport synth√©tique", report, height=300)
                
                # Bouton de t√©l√©chargement du rapport
                st.download_button(
                    label="üì• T√©l√©charger le rapport",
                    data=report,
                    file_name=f"rapport_synthetique_{time.strftime('%Y%m%d_%H%M%S')}.md",
                    mime="text/markdown"
                )
                
            except Exception as e:
                st.error(f"Erreur g√©n√©ration rapport: {e}")

# --- Footer avec monitoring ---
st.markdown("---")
footer_col1, footer_col2, footer_col3 = st.columns(3)

with footer_col1:
    st.caption(f"üìä Donn√©es charg√©es: {time.strftime('%H:%M:%S')}")

with footer_col2:
    st.caption(f"üíæ RAM: {system_metrics['memory_percent']:.1f}%")

with footer_col3:
    if st.button("üîÑ Rafra√Æchir la page"):
        st.rerun()

# Message d'aide
with st.expander("‚ÑπÔ∏è Aide et Informations", expanded=False):
    st.markdown("""
    **Guide d'utilisation :**
    - **Comparaison Globale** : Vue d'ensemble des performances de tous les mod√®les
    - **Analyse Individuelle** : D√©tails sp√©cifiques √† chaque mod√®le
    - **D√©tails Techniques** : Configuration et m√©triques avanc√©es
    - **Export & Rapport** : G√©n√©ration de rapports et exports
    
    **Conseils :**
    - Utilisez l'export JSON pour une analyse approfondie
    - Consultez les m√©triques d√©taill√©es pour comprendre les performances
    - T√©l√©chargez les rapports pour documentation
    """)