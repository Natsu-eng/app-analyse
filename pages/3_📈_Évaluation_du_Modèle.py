import numpy as np
import streamlit as st
import pandas as pd
import time
import json
import gc
import plotly.graph_objects as go
from ml.evaluation.visualization import ModelEvaluationVisualizer
from ml.evaluation.metrics_calculation import get_system_metrics
from utils.report_generator import generate_pdf_report

# --- Configuration de la page ---
st.set_page_config(
    page_title="Ã‰valuation des ModÃ¨les",
    page_icon="ğŸ“ˆ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- CSS personnalisÃ© moderne ---
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: 700;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 15px;
        color: white;
        margin: 0.5rem 0;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .best-model-card {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        border: 2px solid #ffd700;
    }
    .performance-high { color: #00d26a; font-weight: 700; }
    .performance-medium { color: #ffb200; font-weight: 700; }
    .performance-low { color: #ff4d4d; font-weight: 700; }
    .tab-content {
        padding: 2rem 1rem;
        background: #f8f9fa;
        border-radius: 10px;
        margin-top: 1rem;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_data(ttl=3600, max_entries=10)
def cached_plot(fig):
    """Cache les figures Plotly avec gestion optimisÃ©e"""
    return fig

def display_metrics_header(validation):
    """Affiche l'en-tÃªte avec les mÃ©triques principales"""
    successful_count = len(validation["successful_models"])
    total_count = validation["results_count"]
    
    st.markdown('<div class="main-header">ğŸ“ˆ Tableau de Bord d\'Ã‰valuation</div>', unsafe_allow_html=True)
    
    # Indicateurs principaux
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        success_rate = (successful_count / total_count * 100) if total_count > 0 else 0
        st.metric("ğŸ“Š Taux de RÃ©ussite", f"{success_rate:.1f}%")
    
    with col2:
        st.metric("âœ… ModÃ¨les ValidÃ©s", successful_count)
    
    with col3:
        st.metric("âŒ Ã‰checs", len(validation["failed_models"]))
    
    with col4:
        if validation["best_model"]:
            st.metric("ğŸ† Meilleur ModÃ¨le", validation["best_model"])
        else:
            st.metric("ğŸ† Meilleur ModÃ¨le", "N/A")
    
    with col5:
        memory_info = get_system_metrics()
        mem_color = "normal" if memory_info['memory_percent'] < 80 else "off"
        st.metric("ğŸ’¾ MÃ©moire UtilisÃ©e", f"{memory_info['memory_percent']:.1f}%", delta=None, delta_color=mem_color)

def create_sidebar(validation, evaluator):
    """CrÃ©e la sidebar avec contrÃ´les et informations"""
    with st.sidebar:
        st.markdown("### ğŸ“‹ Informations Globales")
        
        # Informations tÃ¢che
        task_type = validation.get('task_type', 'Inconnu')
        task_icon = "ğŸ”®" if task_type == 'clustering' else "ğŸ¯" if task_type == 'classification' else "ğŸ“Š"
        st.write(f"**{task_icon} Type de tÃ¢che:** {task_type}")
        
        if validation["best_model"]:
            st.write(f"**ğŸ† Meilleur modÃ¨le:** `{validation['best_model']}`")
        
        st.write(f"**ğŸ“… DerniÃ¨re mise Ã  jour:** {time.strftime('%H:%M:%S')}")
        
        st.markdown("---")
        st.markdown("### âš™ï¸ ContrÃ´les")
        
        # Boutons d'action
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ”„ Actualiser", use_container_width=True):
                st.rerun()
        with col2:
            if st.button("ğŸ§¹ Optimiser MÃ©moire", use_container_width=True):
                gc.collect()
                st.success("MÃ©moire optimisÃ©e!")
        
        st.markdown("---")
        st.markdown("### ğŸ’¾ Export des RÃ©sultats")
        
        # Export des donnÃ©es
        if validation["successful_models"]:
            export_data = evaluator.get_export_data()
            
            # Export CSV
            csv_data = pd.DataFrame(export_data['models']).to_csv(index=False)
            st.download_button(
                label="ğŸ“„ Exporter CSV",
                data=csv_data,
                file_name=f"evaluation_{task_type}_{int(time.time())}.csv",
                mime="text/csv",
                use_container_width=True
            )
            
            # Export PDF du meilleur modÃ¨le
            if validation["best_model"]:
                best_model_result = next((r for r in validation["successful_models"] 
                                       if evaluator._safe_get(r, ['model_name'], '') == validation["best_model"]), None)
                if best_model_result:
                    try:
                        pdf_bytes = generate_pdf_report(best_model_result)
                        if pdf_bytes:
                            st.download_button(
                                label="ğŸ¯ Rapport PDF",
                                data=pdf_bytes,
                                file_name=f"rapport_{validation['best_model']}_{int(time.time())}.pdf",
                                mime="application/pdf",
                                use_container_width=True
                            )
                    except Exception as e:
                        st.error("Erreur gÃ©nÃ©ration PDF")

def main():
    # VÃ©rification des donnÃ©es
    if 'ml_results' not in st.session_state or not st.session_state.ml_results:
        st.error("ğŸš« Aucun rÃ©sultat d'expÃ©rimentation disponible")
        st.info("Veuillez d'abord entraÃ®ner des modÃ¨les dans la page 'Configuration ML'")
        
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button("âš™ï¸ Aller Ã  la Configuration ML", use_container_width=True):
                st.switch_page("pages/2_âš™ï¸_Configuration_ML.py")
        return

    # Initialisation de l'Ã©valuateur
    try:
        evaluator = ModelEvaluationVisualizer(st.session_state.ml_results)
        validation = evaluator.validation_result
    except Exception as e:
        st.error(f"âŒ Erreur lors de l'initialisation de l'Ã©valuation: {str(e)}")
        return

    if not validation["has_results"]:
        st.error("ğŸ“­ Aucune donnÃ©e d'Ã©valuation valide")
        return

    # En-tÃªte principal
    display_metrics_header(validation)
    
    # Sidebar
    create_sidebar(validation, evaluator)

    # Onglets principaux
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ“Š Vue d'Ensemble", 
        "ğŸ” Analyse DÃ©taillÃ©e", 
        "ğŸ“ˆ MÃ©triques AvancÃ©es",
        "ğŸ’¾ Export Complet"
    ])

    with tab1:
        st.markdown("### ğŸ“Š Comparaison des Performances")
        
        if validation["successful_models"]:
            comparison_plot = evaluator.get_comparison_plot()
            if comparison_plot:
                st.plotly_chart(cached_plot(comparison_plot), use_container_width=True)
            
            st.markdown("### ğŸ“‹ Tableau Comparatif")
            df_comparison = evaluator.get_comparison_dataframe()
            st.dataframe(df_comparison, use_container_width=True, height=400)
            
            # Statistiques rapides
            if validation["task_type"] in ['classification', 'regression']:
                col1, col2, col3 = st.columns(3)
                with col1:
                    avg_metric = df_comparison.select_dtypes(include=[np.number]).iloc[:, 0].mean()
                    st.metric("Moyenne Principale", f"{avg_metric:.3f}")
                with col2:
                    best_metric = df_comparison.select_dtypes(include=[np.number]).iloc[:, 0].max()
                    st.metric("Meilleur Score", f"{best_metric:.3f}")
                with col3:
                    std_metric = df_comparison.select_dtypes(include=[np.number]).iloc[:, 0].std()
                    st.metric("Ã‰cart-type", f"{std_metric:.3f}")
        else:
            st.warning("âš ï¸ Aucun modÃ¨le valide Ã  afficher")

    with tab2:
        st.markdown("### ğŸ” Analyse par ModÃ¨le")
        
        if validation["successful_models"]:
            model_names = [evaluator._safe_get(r, ['model_name'], f'ModÃ¨le_{i}') 
                         for i, r in enumerate(validation["successful_models"])]
            
            # SÃ©lecteur de modÃ¨le avec indicateur du meilleur
            selected_idx = st.selectbox(
                "SÃ©lectionnez un modÃ¨le Ã  analyser:",
                range(len(model_names)),
                format_func=lambda x: f"{model_names[x]} {'ğŸ†' if model_names[x]==validation.get('best_model') else ''}",
                key="model_selector_detail"
            )
            
            selected_result = validation["successful_models"][selected_idx]
            evaluator.show_model_details(selected_result, validation["task_type"])
            
        else:
            st.info("â„¹ï¸ Aucun modÃ¨le disponible pour l'analyse dÃ©taillÃ©e")

    with tab3:
        st.markdown("### ğŸ“ˆ Analyses AvancÃ©es")
        
        if validation["successful_models"]:
            # Distribution des performances
            dist_plot = evaluator.get_performance_distribution_plot()
            if dist_plot:
                st.plotly_chart(cached_plot(dist_plot), use_container_width=True)
            
            # MÃ©triques dÃ©taillÃ©es selon le type de tÃ¢che
            if validation["task_type"] == 'clustering':
                st.markdown("#### ğŸ¯ MÃ©triques de Clustering")
                clustering_metrics = []
                for result in validation["successful_models"]:
                    metrics = evaluator._safe_get(result, ['metrics'], {})
                    clustering_metrics.append({
                        'ModÃ¨le': evaluator._safe_get(result, ['model_name'], 'Unknown'),
                        'Silhouette': metrics.get('silhouette_score', 'N/A'),
                        'Clusters': metrics.get('n_clusters', 'N/A'),
                        'StabilitÃ©': 'Ã‰levÃ©e' if metrics.get('silhouette_score', 0) > 0.5 else 'Moyenne' if metrics.get('silhouette_score', 0) > 0.3 else 'Faible'
                    })
                st.dataframe(pd.DataFrame(clustering_metrics), use_container_width=True)
                
        else:
            st.warning("âš ï¸ Aucune mÃ©trique avancÃ©e disponible")

    with tab4:
        st.markdown("### ğŸ’¾ Export des RÃ©sultats Complets")
        
        if validation["successful_models"]:
            export_data = evaluator.get_export_data()
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### ğŸ“Š Format JSON")
                st.download_button(
                    label="ğŸ“¥ TÃ©lÃ©charger JSON",
                    data=json.dumps(export_data, indent=2, ensure_ascii=False, default=str),
                    file_name=f"evaluation_complete_{int(time.time())}.json",
                    mime="application/json",
                    use_container_width=True
                )
                
                # AperÃ§u des donnÃ©es
                with st.expander("ğŸ‘ï¸ AperÃ§u des donnÃ©es exportÃ©es"):
                    st.json(export_data, expanded=False)
            
            with col2:
                st.markdown("#### ğŸ“ˆ Rapport DÃ©taillÃ©")
                st.info("Exportez un rapport complet avec toutes les visualisations et analyses")
                
                # Bouton pour gÃ©nÃ©rer un rapport complet
                if st.button("ğŸ”„ GÃ©nÃ©rer Rapport Complet", use_container_width=True):
                    with st.spinner("GÃ©nÃ©ration du rapport en cours..."):
                        try:
                            # Ici vous pouvez Ã©tendre pour gÃ©nÃ©rer un rapport plus complet
                            st.success("âœ… Rapport gÃ©nÃ©rÃ© avec succÃ¨s!")
                        except Exception as e:
                            st.error(f"âŒ Erreur lors de la gÃ©nÃ©ration: {str(e)}")
        else:
            st.info("â„¹ï¸ Aucune donnÃ©e Ã  exporter")

    # Pied de page avec informations systÃ¨me
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.caption(f"ğŸ• DerniÃ¨re mise Ã  jour: {time.strftime('%H:%M:%S')}")
    with col2:
        memory_info = get_system_metrics()
        st.caption(f"ğŸ’¾ MÃ©moire: {memory_info['memory_percent']:.1f}%")
    with col3:
        st.caption(f"ğŸ“Š ModÃ¨les chargÃ©s: {len(st.session_state.ml_results)}")

if __name__ == "__main__":
    main()