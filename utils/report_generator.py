import contextlib
import io
import tempfile
import numpy as np
import plotly.io as pio
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.pagesizes import A4, letter
from reportlab.platypus import (
    SimpleDocTemplate, Paragraph, Spacer, Image, PageBreak, Table, TableStyle
)
from reportlab.lib.units import inch
from reportlab.lib import colors
from ml.evaluation.visualization import ModelEvaluationVisualizer
import logging
from datetime import datetime
import gc
import os
import pandas as pd
from typing import Dict, Any, Optional
import base64
from io import BytesIO

logger = logging.getLogger(__name__)

# --- Styles personnalis√©s modernis√©s ---
def _create_custom_styles():
    """Cr√©e des styles modernes pour le PDF"""
    styles = getSampleStyleSheet()
    
    # V√©rifier et ajouter les styles seulement s'ils n'existent pas d√©j√†
    custom_styles = {}
    
    if 'MainTitle' not in styles:
        styles.add(ParagraphStyle(
            name='MainTitle',
            parent=styles['Title'],
            fontSize=20,
            textColor=colors.HexColor('#1f77b4'),
            spaceAfter=30,
            alignment=1,
            fontName='Helvetica-Bold'
        ))
    
    if 'SubTitle' not in styles:
        styles.add(ParagraphStyle(
            name='SubTitle',
            parent=styles['Heading2'],
            fontSize=14,
            textColor=colors.HexColor('#2ca02c'),
            spaceAfter=15,
            fontName='Helvetica-Bold'
        ))
    
    if 'MetricHighlight' not in styles:
        styles.add(ParagraphStyle(
            name='MetricHighlight',
            parent=styles['Normal'],
            fontSize=12,
            textColor=colors.HexColor('#d62728'),
            spaceAfter=6,
            fontName='Helvetica-Bold'
        ))
    
    # Utiliser un nom unique pour √©viter les conflits
    if 'BodyTextCustom' not in styles:
        styles.add(ParagraphStyle(
            name='BodyTextCustom',
            parent=styles['Normal'],
            fontSize=10,
            textColor=colors.HexColor('#333333'),
            spaceAfter=12,
            fontName='Helvetica'
        ))
    
    return styles

CUSTOM_STYLES = _create_custom_styles()

# --- En-t√™te et pied de page modernis√©s ---
def _header(canvas, doc, model_name: str = ""):
    """En-t√™te moderne avec informations contextuelles"""
    canvas.saveState()
    
    # Fond d'en-t√™te
    canvas.setFillColor(colors.HexColor('#f8f9fa'))
    canvas.rect(0, doc.height + doc.topMargin - 0.7*inch, doc.width + 2*inch, 0.7*inch, fill=1, stroke=0)
    
    # Texte
    canvas.setFont('Helvetica-Bold', 10)
    canvas.setFillColor(colors.HexColor('#1f77b4'))
    canvas.drawString(inch, doc.height + doc.topMargin - 0.4*inch, 
                     f"Rapport ML - {model_name}" if model_name else "Rapport d'√âvaluation ML")
    
    canvas.setFont('Helvetica', 8)
    canvas.setFillColor(colors.HexColor('#666666'))
    canvas.drawRightString(doc.width + inch, doc.height + doc.topMargin - 0.4*inch, 
                          f"G√©n√©r√© le: {datetime.now().strftime('%d/%m/%Y %H:%M')}")
    
    canvas.restoreState()

def _footer(canvas, doc):
    """Pied de page professionnel"""
    canvas.saveState()
    
    # Ligne de s√©paration
    canvas.setStrokeColor(colors.HexColor('#e9ecef'))
    canvas.setLineWidth(0.5)
    canvas.line(inch, 0.7*inch, doc.width + inch, 0.7*inch)
    
    # Texte
    canvas.setFont('Helvetica', 8)
    canvas.setFillColor(colors.HexColor('#666666'))
    canvas.drawString(inch, 0.5*inch, f"Page {doc.page}")
    canvas.drawRightString(doc.width + inch, 0.5*inch, "Document confidentiel - Plateforme ML")
    
    canvas.restoreState()

# --- Conversion graphiques Plotly optimis√©e ---
def _plotly_fig_to_pdf_image(fig, width=6*inch, height=4*inch, dpi=150) -> Image:
    """Convertit une figure Plotly en image PDF avec gestion d'erreur robuste"""
    try:
        if fig is None:
            return Paragraph("Graphique non disponible", CUSTOM_STYLES['BodyText'])
        
        # Conversion haute qualit√©
        img_bytes = pio.to_image(
            fig, 
            format='png', 
            width=int(width / inch * dpi),
            height=int(height / inch * dpi),
            scale=2,
            engine='kaleido'
        )
        
        return Image(BytesIO(img_bytes), width=width, height=height)
        
    except Exception as e:
        logger.error(f"Erreur conversion graphique: {e}")
        error_msg = f"Erreur affichage graphique: {str(e)[:100]}..."
        return Paragraph(error_msg, CUSTOM_STYLES['BodyText'])

# --- Page de couverture moderne ---
def _build_cover_page(story, model_result: Dict[str, Any]):
    """Page de couverture professionnelle"""
    model_name = model_result.get('model_name', 'Mod√®le Inconnu')
    task_type = model_result.get('task_type', 'inconnu')
    metrics = model_result.get('metrics', {})
    
    # Titre principal
    story.append(Spacer(1, 2*inch))
    story.append(Paragraph("RAPPORT D'√âVALUATION", CUSTOM_STYLES['MainTitle']))
    story.append(Paragraph("PLATEFORME MACHINE LEARNING", CUSTOM_STYLES['SubTitle']))
    story.append(Spacer(1, 0.5*inch))
    
    # Carte d'information moderne
    info_data = [
        ["üîÆ Mod√®le", model_name],
        ["üéØ Type de t√¢che", task_type.upper()],
        ["üìÖ Date", datetime.now().strftime('%d/%m/%Y %H:%M:%S')],
        ["‚è±Ô∏è Temps d'entra√Ænement", f"{model_result.get('training_time', 0):.2f}s"],
    ]
    
    # Ajout m√©trique principale selon le type
    if task_type == 'classification' and 'accuracy' in metrics:
        accuracy = metrics.get('accuracy', 0)
        info_data.append(["üìä Accuracy", f"{accuracy:.3f}"])
    elif task_type == 'regression' and 'r2' in metrics:
        r2 = metrics.get('r2', 0)
        info_data.append(["üìà Score R¬≤", f"{r2:.3f}"])
    elif task_type == 'clustering' and 'silhouette_score' in metrics:
        silhouette = metrics.get('silhouette_score', 0)
        info_data.append(["üé® Score Silhouette", f"{silhouette:.3f}"])
    
    info_table = Table(info_data, colWidths=[2*inch, 3*inch])
    info_table.setStyle(TableStyle([
        ('FONT', (0, 0), (-1, -1), 'Helvetica', 10),
        ('FONT', (0, 0), (0, -1), 'Helvetica-Bold', 10),
        ('BACKGROUND', (0, 0), (0, -1), colors.HexColor('#e3f2fd')),
        ('BACKGROUND', (1, 0), (1, -1), colors.HexColor('#f8f9fa')),
        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
        ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor('#dee2e6')),
        ('PADDING', (0, 0), (-1, -1), 10),
        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
    ]))
    
    story.append(info_table)
    story.append(PageBreak())

# --- Section m√©triques modernis√©e ---
def _add_metrics_section(story, model_result: Dict[str, Any]):
    """Section m√©triques avec design moderne"""
    task_type = model_result.get('task_type', 'inconnu')
    metrics = model_result.get('metrics', {})
    
    story.append(Paragraph("üìä M√âTRIQUES DE PERFORMANCE", CUSTOM_STYLES['SubTitle']))
    story.append(Spacer(1, 0.2*inch))
    
    if task_type == 'classification':
        _add_classification_metrics(story, metrics)
    elif task_type == 'regression':
        _add_regression_metrics(story, metrics)
    elif task_type == 'clustering':
        _add_clustering_metrics(story, metrics)
    else:
        story.append(Paragraph("Type de t√¢che non reconnu", CUSTOM_STYLES['BodyText']))
    
    story.append(Spacer(1, 0.3*inch))

def _add_classification_metrics(story, metrics: Dict[str, Any]):
    """M√©triques de classification avec indicateurs visuels"""
    metric_data = [
        ['M√©trique', 'Valeur', 'Interpr√©tation'],
        ['Accuracy', f"{metrics.get('accuracy', 0):.3f}", _get_interpretation('accuracy', metrics.get('accuracy', 0))],
        ['F1-Score', f"{metrics.get('f1_score', 0):.3f}", _get_interpretation('f1_score', metrics.get('f1_score', 0))],
        ['Pr√©cision', f"{metrics.get('precision', 0):.3f}", _get_interpretation('precision', metrics.get('precision', 0))],
        ['Rappel', f"{metrics.get('recall', 0):.3f}", _get_interpretation('recall', metrics.get('recall', 0))],
    ]
    
    if 'roc_auc' in metrics:
        metric_data.append(['AUC ROC', f"{metrics.get('roc_auc', 0):.3f}", _get_interpretation('roc_auc', metrics.get('roc_auc', 0))])
    
    metric_table = Table(metric_data, colWidths=[1.8*inch, 1*inch, 2.2*inch])
    metric_table.setStyle(TableStyle([
        ('FONT', (0, 0), (-1, 0), 'Helvetica-Bold', 10),
        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#343a40')),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor('#adb5bd')),
        ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor('#f8f9fa')),
        ('FONT', (0, 1), (-1, -1), 'Helvetica', 9),
    ]))
    story.append(metric_table)

def _add_regression_metrics(story, metrics: Dict[str, Any]):
    """M√©triques de r√©gression avec indicateurs"""
    metric_data = [
        ['M√©trique', 'Valeur', 'Interpr√©tation'],
        ['R¬≤', f"{metrics.get('r2', 0):.3f}", _get_interpretation('r2', metrics.get('r2', 0))],
        ['MAE', f"{metrics.get('mae', 0):.3f}", _get_interpretation('mae', metrics.get('mae', 0))],
        ['RMSE', f"{metrics.get('rmse', 0):.3f}", _get_interpretation('rmse', metrics.get('rmse', 0))],
    ]
    
    metric_table = Table(metric_data, colWidths=[1.8*inch, 1*inch, 2.2*inch])
    metric_table.setStyle(TableStyle([
        ('FONT', (0, 0), (-1, 0), 'Helvetica-Bold', 10),
        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#198754')),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor('#adb5bd')),
        ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor('#f8f9fa')),
        ('FONT', (0, 1), (-1, -1), 'Helvetica', 9),
    ]))
    story.append(metric_table)

def _add_clustering_metrics(story, metrics: Dict[str, Any]):
    """M√©triques de clustering modernis√©es"""
    metric_data = [
        ['M√©trique', 'Valeur', 'Interpr√©tation'],
        ['Silhouette', f"{metrics.get('silhouette_score', 0):.3f}", _get_interpretation('silhouette_score', metrics.get('silhouette_score', 0))],
        ['Clusters', f"{metrics.get('n_clusters', 0)}", _get_interpretation('n_clusters', metrics.get('n_clusters', 0))],
    ]
    
    if 'n_outliers' in metrics:
        metric_data.append(['Outliers', f"{metrics.get('n_outliers', 0)}", 'Points de bruit d√©tect√©s'])
    
    metric_table = Table(metric_data, colWidths=[1.8*inch, 1*inch, 2.2*inch])
    metric_table.setStyle(TableStyle([
        ('FONT', (0, 0), (-1, 0), 'Helvetica-Bold', 10),
        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#6f42c1')),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor('#adb5bd')),
        ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor('#f8f9fa')),
        ('FONT', (0, 1), (-1, -1), 'Helvetica', 9),
    ]))
    story.append(metric_table)

def _get_interpretation(metric: str, value: float) -> str:
    """Retourne l'interpr√©tation d'une m√©trique"""
    interpretations = {
        'accuracy': {
            (0.9, 1.0): 'Excellent',
            (0.8, 0.9): 'Tr√®s bon',
            (0.7, 0.8): 'Bon',
            (0.6, 0.7): 'Moyen',
            (0.0, 0.6): '√Ä am√©liorer'
        },
        'f1_score': {
            (0.9, 1.0): 'Excellent',
            (0.8, 0.9): 'Tr√®s bon',
            (0.7, 0.8): 'Bon',
            (0.6, 0.7): 'Moyen',
            (0.0, 0.6): '√Ä am√©liorer'
        },
        'r2': {
            (0.9, 1.0): 'Excellente explication',
            (0.7, 0.9): 'Bonne explication',
            (0.5, 0.7): 'Explication mod√©r√©e',
            (0.3, 0.5): 'Faible explication',
            (0.0, 0.3): 'Tr√®s faible explication'
        },
        'silhouette_score': {
            (0.7, 1.0): 'Clusters excellents',
            (0.5, 0.7): 'Clusters raisonnables',
            (0.3, 0.5): 'Clusters faibles',
            (0.0, 0.3): 'Clusters douteux',
            (-1.0, 0.0): 'Clusters inad√©quats'
        }
    }
    
    if metric in interpretations:
        for range_val, interpretation in interpretations[metric].items():
            if range_val[0] <= value <= range_val[1]:
                return interpretation
    
    return 'Non √©valu√©'

# --- Section configuration du mod√®le ---
def _add_model_configuration(story, model_result: Dict[str, Any]):
    """Section configuration du mod√®le"""
    story.append(Paragraph("‚öôÔ∏è CONFIGURATION DU MOD√àLE", CUSTOM_STYLES['SubTitle']))
    story.append(Spacer(1, 0.2*inch))
    
    config_data = [
        ["Param√®tre", "Valeur"],
        ["Nom du mod√®le", model_result.get('model_name', 'N/A')],
        ["Type de t√¢che", model_result.get('task_type', 'N/A')],
        ["Temps d'entra√Ænement", f"{model_result.get('training_time', 0):.2f} secondes"],
    ]
    
    # Param√®tres optimis√©s
    best_params = model_result.get('best_params', {})
    if best_params:
        for param, value in list(best_params.items())[:8]:  # Limite √† 8 param√®tres
            config_data.append([f"üîß {param}", str(value)[:50] + '...' if len(str(value)) > 50 else str(value)])
    
    config_table = Table(config_data, colWidths=[2.5*inch, 3*inch])
    config_table.setStyle(TableStyle([
        ('FONT', (0, 0), (-1, 0), 'Helvetica-Bold', 10),
        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#6c757d')),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
        ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor('#dee2e6')),
        ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor('#ffffff')),
        ('FONT', (0, 1), (-1, -1), 'Helvetica', 9),
    ]))
    
    story.append(config_table)
    story.append(Spacer(1, 0.3*inch))

# --- Section visualisations adaptative ---
def _add_visualizations_section(story, model_result: Dict[str, Any], evaluator: ModelEvaluationVisualizer):
    """Section visualisations adapt√©e au type de t√¢che"""
    story.append(PageBreak())
    story.append(Paragraph("üìà VISUALISATIONS", CUSTOM_STYLES['SubTitle']))
    story.append(Spacer(1, 0.3*inch))
    
    task_type = model_result.get('task_type', 'inconnu')
    
    try:
        if task_type == 'clustering':
            _add_clustering_visualizations(story, model_result, evaluator)
        elif task_type == 'classification':
            _add_classification_visualizations(story, model_result, evaluator)
        elif task_type == 'regression':
            _add_regression_visualizations(story, model_result, evaluator)
        else:
            story.append(Paragraph("Visualisations non disponibles pour ce type de t√¢che.", CUSTOM_STYLES['BodyText']))
    except Exception as e:
        logger.error(f"Erreur g√©n√©ration visualisations: {e}")
        story.append(Paragraph(f"‚ùå Erreur lors de la g√©n√©ration des visualisations: {str(e)}", CUSTOM_STYLES['BodyText']))

def _add_clustering_visualizations(story, model_result: Dict[str, Any], evaluator: ModelEvaluationVisualizer):
    """Visualisations pour le clustering"""
    # Graphique des clusters
    story.append(Paragraph("üéØ Visualisation des Clusters", CUSTOM_STYLES['BodyText']))
    try:
        cluster_fig = evaluator.create_cluster_scatter_plot(model_result)
        if cluster_fig:
            story.append(_plotly_fig_to_pdf_image(cluster_fig, width=6*inch, height=4*inch))
        else:
            story.append(Paragraph("Graphique des clusters non disponible", CUSTOM_STYLES['BodyText']))
    except Exception as e:
        logger.error(f"Erreur graphique clusters: {e}")
        story.append(Paragraph("‚ùå Erreur g√©n√©ration graphique clusters", CUSTOM_STYLES['BodyText']))
    
    story.append(Spacer(1, 0.2*inch))
    
    # Graphique silhouette
    story.append(Paragraph("üìä Analyse Silhouette", CUSTOM_STYLES['BodyText']))
    try:
        silhouette_fig = evaluator.create_silhouette_plot(model_result)
        if silhouette_fig:
            story.append(_plotly_fig_to_pdf_image(silhouette_fig, width=6*inch, height=4*inch))
        else:
            story.append(Paragraph("Graphique silhouette non disponible", CUSTOM_STYLES['BodyText']))
    except Exception as e:
        logger.error(f"Erreur graphique silhouette: {e}")
        story.append(Paragraph("‚ùå Erreur g√©n√©ration graphique silhouette", CUSTOM_STYLES['BodyText']))

def _add_classification_visualizations(story, model_result: Dict[str, Any], evaluator: ModelEvaluationVisualizer):
    """Visualisations pour la classification"""
    story.append(Paragraph("üìä M√©triques de Classification", CUSTOM_STYLES['BodyText']))
    # Ajouter d'autres visualisations sp√©cifiques √† la classification si n√©cessaire
    story.append(Paragraph("Visualisations d√©taill√©es disponibles dans l'interface web.", CUSTOM_STYLES['BodyText']))

def _add_regression_visualizations(story, model_result: Dict[str, Any], evaluator: ModelEvaluationVisualizer):
    """Visualisations pour la r√©gression"""
    story.append(Paragraph("üìà M√©triques de R√©gression", CUSTOM_STYLES['BodyText']))
    # Ajouter d'autres visualisations sp√©cifiques √† la r√©gression si n√©cessaire
    story.append(Paragraph("Visualisations d√©taill√©es disponibles dans l'interface web.", CUSTOM_STYLES['BodyText']))

# --- Section conclusion ---
def _add_conclusion_section(story, model_result: Dict[str, Any]):
    """Section conclusion avec recommandations"""
    story.append(PageBreak())
    story.append(Paragraph("üéØ CONCLUSION ET RECOMMANDATIONS", CUSTOM_STYLES['SubTitle']))
    story.append(Spacer(1, 0.2*inch))
    
    task_type = model_result.get('task_type', 'inconnu')
    metrics = model_result.get('metrics', {})
    model_name = model_result.get('model_name', 'Mod√®le')
    
    # √âvaluation globale
    evaluation_text = f"Le mod√®le {model_name} a d√©montr√© des performances "
    
    if task_type == 'classification':
        accuracy = metrics.get('accuracy', 0)
        if accuracy >= 0.9:
            evaluation_text += "excellentes en classification avec une pr√©cision tr√®s √©lev√©e."
        elif accuracy >= 0.8:
            evaluation_text += "satisfaisantes en classification."
        else:
            evaluation_text += "n√©cessitant des am√©liorations en classification."
    
    elif task_type == 'regression':
        r2 = metrics.get('r2', 0)
        if r2 >= 0.8:
            evaluation_text += "excellentes en r√©gression avec une forte capacit√© explicative."
        elif r2 >= 0.6:
            evaluation_text += "correctes en r√©gression."
        else:
            evaluation_text += "limit√©es en r√©gression."
    
    elif task_type == 'clustering':
        silhouette = metrics.get('silhouette_score', 0)
        if silhouette >= 0.7:
            evaluation_text += "excellentes en clustering avec une s√©paration nette des groupes."
        elif silhouette >= 0.5:
            evaluation_text += "satisfaisantes en clustering."
        else:
            evaluation_text += "n√©cessitant une optimisation en clustering."
    
    story.append(Paragraph(evaluation_text, CUSTOM_STYLES['BodyText']))
    story.append(Spacer(1, 0.2*inch))
    
    # Recommandations
    story.append(Paragraph("üí° Recommandations:", CUSTOM_STYLES['BodyText']))
    recommendations = [
        "‚Ä¢ V√©rifier la qualit√© des donn√©es d'entra√Ænement",
        "‚Ä¢ Exp√©rimenter avec d'autres algorithmes",
        "‚Ä¢ Optimiser les hyperparam√®tres",
        "‚Ä¢ Valider sur de nouvelles donn√©es",
        "‚Ä¢ Surveiller les performances en production"
    ]
    
    for rec in recommendations:
        story.append(Paragraph(rec, CUSTOM_STYLES['BodyText']))

# --- Gestionnaire de contexte pour les fichiers temporaires ---
@contextlib.contextmanager
def _temp_directory_manager():
    """Gestionnaire de r√©pertoire temporaire"""
    temp_dir = tempfile.mkdtemp()
    try:
        yield temp_dir
    finally:
        try:
            import shutil
            shutil.rmtree(temp_dir)
            logger.info("R√©pertoire temporaire nettoy√©")
        except Exception as e:
            logger.warning(f"Impossible de nettoyer le r√©pertoire temporaire: {e}")

# --- Fonction principale de g√©n√©ration de PDF ---
def generate_pdf_report(model_result: Dict[str, Any]) -> Optional[bytes]:
    """
    G√©n√®re un rapport PDF professionnel pour un mod√®le ML
    
    Args:
        model_result: R√©sultats du mod√®le au format dictionnaire
        
    Returns:
        bytes: Contenu du PDF ou None en cas d'erreur
    """
    if not model_result or not isinstance(model_result, dict):
        logger.error("Donn√©es du mod√®le invalides pour la g√©n√©ration du PDF")
        return None
    
    try:
        # Initialisation du buffer
        buffer = io.BytesIO()
        
        # Configuration du document
        doc = SimpleDocTemplate(
            buffer,
            pagesize=A4,
            topMargin=0.7*inch,
            bottomMargin=0.7*inch,
            leftMargin=0.7*inch,
            rightMargin=0.7*inch
        )
        
        story = []
        model_name = model_result.get('model_name', 'Mod√®le')
        
        # Initialisation de l'√©valuateur
        evaluator = ModelEvaluationVisualizer([model_result])
        
        # Construction du rapport
        _build_cover_page(story, model_result)
        _add_metrics_section(story, model_result)
        _add_model_configuration(story, model_result)
        _add_visualizations_section(story, model_result, evaluator)
        _add_conclusion_section(story, model_result)
        
        # G√©n√©ration du PDF
        logger.info(f"G√©n√©ration du PDF pour {model_name} avec {len(story)} √©l√©ments")
        
        doc.build(
            story,
            onFirstPage=lambda c, d: _header(c, d, model_name),
            onLaterPages=lambda c, d: _header(c, d, model_name)
        )
        
        # R√©cup√©ration des bytes
        pdf_bytes = buffer.getvalue()
        buffer.close()
        
        logger.info(f"‚úÖ PDF g√©n√©r√© avec succ√®s pour {model_name} ({len(pdf_bytes)} bytes)")
        return pdf_bytes
        
    except Exception as e:
        logger.error(f"‚ùå Erreur critique lors de la g√©n√©ration du PDF: {e}")
        try:
            if 'buffer' in locals():
                buffer.close()
        except:
            pass
        return None

# --- Version simplifi√©e pour tests ---
def generate_basic_pdf_report(model_result: Dict[str, Any]) -> Optional[bytes]:
    """Version simplifi√©e pour tests rapides"""
    try:
        buffer = io.BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=A4)
        story = []
        
        story.append(Paragraph("Rapport ML Simplifi√©", CUSTOM_STYLES['MainTitle']))
        story.append(Spacer(1, 0.5*inch))
        story.append(Paragraph(f"Mod√®le: {model_result.get('model_name', 'N/A')}", CUSTOM_STYLES['BodyText']))
        story.append(Paragraph(f"Type: {model_result.get('task_type', 'N/A')}", CUSTOM_STYLES['BodyText']))
        
        doc.build(story)
        pdf_bytes = buffer.getvalue()
        buffer.close()
        
        return pdf_bytes
        
    except Exception as e:
        logger.error(f"Erreur PDF basique: {e}")
        return None

# --- Fonction utilitaire pour pr√©visualisation ---
def get_report_summary(model_result: Dict[str, Any]) -> Dict[str, Any]:
    """Retourne un r√©sum√© du rapport"""
    if not model_result:
        return {"error": "Donn√©es manquantes"}
    
    metrics = model_result.get('metrics', {})
    task_type = model_result.get('task_type', 'inconnu')
    
    summary = {
        "model_name": model_result.get('model_name', 'Inconnu'),
        "task_type": task_type,
        "training_time": model_result.get('training_time', 0),
        "main_metric": None,
        "metric_value": 0,
        "status": "inconnu"
    }
    
    # D√©termination de la m√©trique principale
    if task_type == 'classification':
        summary["main_metric"] = "accuracy"
        summary["metric_value"] = metrics.get('accuracy', 0)
    elif task_type == 'regression':
        summary["main_metric"] = "r2"
        summary["metric_value"] = metrics.get('r2', 0)
    elif task_type == 'clustering':
        summary["main_metric"] = "silhouette_score"
        summary["metric_value"] = metrics.get('silhouette_score', 0)
    
    # Statut de performance
    if summary["metric_value"] is not None:
        if summary["metric_value"] > 0.8:
            summary["status"] = "excellent"
        elif summary["metric_value"] > 0.6:
            summary["status"] = "bon"
        else:
            summary["status"] = "√† am√©liorer"
    
    return summary