"""
Module d'entra√Ænement robuste pour le machine learning.
Supporte l'apprentissage supervis√© et non-supervis√© avec gestion MLOps avanc√©e.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, KFold
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
import joblib
import os
import time
import gc
import psutil
from typing import Dict, List, Any, Optional, Tuple
import warnings
from sklearn.exceptions import ConvergenceWarning
import json
from datetime import datetime

# Int√©gration MLflow
try:
    import mlflow
    import mlflow.sklearn
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    mlflow = None

# Configuration des warnings
warnings.filterwarnings("ignore", category=ConvergenceWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# Import des modules de l'application
try:
    from src.models.catalog import get_model_config
    from src.data.preprocessing import create_preprocessor, safe_label_encode, validate_preprocessor
    from src.evaluation.metrics import EvaluationMetrics
    from src.data.data_analysis import auto_detect_column_types
    from src.shared.logging import get_logger
    from src.config.constants import TRAINING_CONSTANTS, PREPROCESSING_CONSTANTS
except ImportError as e:
    print(f"Warning: Some imports failed - {e}")
    # Fallback pour les tests
    def get_model_config(*args, **kwargs): return None
    def auto_detect_column_types(*args, **kwargs): return {}
    def get_logger(name): 
        import logging
        logging.basicConfig(level=logging.INFO)
        return logging.getLogger(name)
    TRAINING_CONSTANTS = {}
    PREPROCESSING_CONSTANTS = {}

logger = get_logger(__name__)


def is_mlflow_available() -> bool:
    """V√©rifie si MLflow est disponible et fonctionnel."""
    return MLFLOW_AVAILABLE and mlflow is not None


class TrainingMonitor:
    """Monitor pour suivre la progression et les ressources pendant l'entra√Ænement."""
    
    def __init__(self):
        self.start_time = None
        self.model_start_time = None
        self.memory_usage = []
        self.current_model = None
        
    def start_training(self) -> None:
        """D√©marre le monitoring de l'entra√Ænement."""
        self.start_time = time.time()
        self.memory_usage = []
        logger.info("üöÄ D√©but du monitoring de l'entra√Ænement")
        
    def start_model(self, model_name: str) -> None:
        """D√©marre le monitoring pour un mod√®le sp√©cifique."""
        self.model_start_time = time.time()
        self.current_model = model_name
        logger.info(f"üîß D√©but de l'entra√Ænement pour: {model_name}")
        
    def check_resources(self) -> Dict[str, Any]:
        """V√©rifie l'utilisation des ressources syst√®me."""
        try:
            memory = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent(interval=1)
            
            resource_info = {
                'memory_percent': memory.percent,
                'memory_used_mb': memory.used / (1024 * 1024),
                'cpu_percent': cpu_percent,
                'timestamp': time.time(),
                'model': self.current_model
            }
            
            self.memory_usage.append(resource_info)
            
            # Alertes si utilisation √©lev√©e
            high_mem_threshold = TRAINING_CONSTANTS.get("HIGH_MEMORY_THRESHOLD", 85)
            high_cpu_threshold = TRAINING_CONSTANTS.get("HIGH_CPU_THRESHOLD", 90)
            
            if memory.percent > high_mem_threshold:
                logger.warning(f"‚ö†Ô∏è Utilisation m√©moire √©lev√©e: {memory.percent:.1f}%")
            if cpu_percent > high_cpu_threshold:
                logger.warning(f"‚ö†Ô∏è Utilisation CPU √©lev√©e: {cpu_percent:.1f}%")
                
            return resource_info
            
        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification ressources: {e}")
            return {}
    
    def get_model_duration(self) -> float:
        """Retourne la dur√©e d'entra√Ænement du mod√®le actuel."""
        if self.model_start_time:
            return time.time() - self.model_start_time
        return 0.0
    
    def get_total_duration(self) -> float:
        """Retourne la dur√©e totale d'entra√Ænement."""
        if self.start_time:
            return time.time() - self.start_time
        return 0.0
    
    def get_summary(self) -> Dict[str, Any]:
        """Retourne un r√©sum√© du monitoring."""
        return {
            'total_duration': self.get_total_duration(),
            'memory_samples': len(self.memory_usage),
            'peak_memory': max([m.get('memory_percent', 0) for m in self.memory_usage]) if self.memory_usage else 0,
            'current_model': self.current_model
        }


def validate_training_data(X: pd.DataFrame, 
                          y: Optional[pd.Series], 
                          task_type: str) -> Dict[str, Any]:
    """
    Valide les donn√©es d'entra√Ænement de fa√ßon robuste.
    
    Args:
        X: Features
        y: Target (peut √™tre None pour unsupervised)
        task_type: Type de t√¢che ML
        
    Returns:
        Dict avec les r√©sultats de validation
    """
    validation = {
        "is_valid": True,
        "issues": [],
        "warnings": [],
        "samples_count": len(X) if X is not None else 0,
        "features_count": len(X.columns) if hasattr(X, 'columns') else (X.shape[1] if X is not None else 0),
        "data_quality": {}
    }
    
    try:
        min_samples = TRAINING_CONSTANTS.get("MIN_SAMPLES_REQUIRED", 10)
        max_missing_ratio = TRAINING_CONSTANTS.get("MAX_MISSING_RATIO", 0.9)
        min_numeric_features = TRAINING_CONSTANTS.get("MIN_NUMERIC_FEATURES", 2)
        max_classes = TRAINING_CONSTANTS.get("MAX_CLASSES", 50)
        
        # V√©rification des dimensions de base
        if X is None or len(X) == 0:
            validation["is_valid"] = False
            validation["issues"].append("Dataset X vide ou None")
            return validation
            
        if len(X) < min_samples:
            validation["is_valid"] = False
            validation["issues"].append(f"Trop peu d'√©chantillons ({len(X)} < {min_samples})")
            
        if validation["features_count"] == 0:
            validation["is_valid"] = False
            validation["issues"].append("Aucune feature disponible")
        
        # V√©rification de la m√©moire
        try:
            if hasattr(X, 'memory_usage'):
                x_memory = X.memory_usage(deep=True).sum() / (1024 * 1024)
                validation["data_quality"]["memory_usage_mb"] = x_memory
                memory_limit = TRAINING_CONSTANTS.get("MEMORY_LIMIT_MB", 1000)
                if x_memory > memory_limit:
                    validation["warnings"].append(f"Dataset volumineux ({x_memory:.1f}MB)")
        except Exception as e:
            logger.debug(f"Erreur calcul m√©moire: {e}")
        
        # V√©rification des valeurs manquantes
        missing_stats = X.isna().sum()
        total_missing = missing_stats.sum()
        missing_ratio = total_missing / (X.shape[0] * X.shape[1]) if X.size > 0 else 0
        
        validation["data_quality"]["total_missing"] = int(total_missing)
        validation["data_quality"]["missing_ratio"] = float(missing_ratio)
        
        if missing_ratio > max_missing_ratio:
            validation["warnings"].append(f"Ratio de valeurs manquantes √©lev√©: {missing_ratio:.1%}")
        
        # V√©rification sp√©cifique au non-supervis√©
        if task_type == 'clustering':
            if y is not None:
                validation["warnings"].append("Target ignor√©e pour le clustering")
            
            numeric_features = X.select_dtypes(include=[np.number]).columns
            validation["data_quality"]["numeric_features"] = len(numeric_features)
            
            if len(numeric_features) < min_numeric_features:
                validation["warnings"].append(f"Peu de features num√©riques ({len(numeric_features)} < {min_numeric_features}) pour le clustering")
        
        # V√©rification de la target pour supervis√©
        elif y is not None:
            if len(y) != len(X):
                validation["is_valid"] = False
                validation["issues"].append(f"Dimensions X et y incoh√©rentes: {len(X)} vs {len(y)}")
                
            valid_target_count = y.notna().sum() if hasattr(y, 'notna') else np.sum(~np.isnan(y))
            validation["data_quality"]["valid_target_count"] = int(valid_target_count)
            
            if valid_target_count < min_samples:
                validation["is_valid"] = False
                validation["issues"].append(f"Trop peu de targets valides ({valid_target_count})")
                
            if task_type == 'classification':
                unique_classes = np.unique(y.dropna()) if hasattr(y, 'dropna') else np.unique(y[~np.isnan(y)])
                n_classes = len(unique_classes)
                validation["data_quality"]["n_classes"] = n_classes
                
                if n_classes < 2:
                    validation["is_valid"] = False
                    validation["issues"].append("Moins de 2 classes distinctes")
                elif n_classes > max_classes:
                    validation["warnings"].append(f"Plus de {max_classes} classes - v√©rifiez la variable cible")
            
        logger.info(f"‚úÖ Validation donn√©es: {validation['samples_count']} √©chantillons, "
                   f"{validation['features_count']} features, {len(validation['issues'])} issues")
        
    except Exception as e:
        validation["is_valid"] = False
        validation["issues"].append(f"Erreur de validation: {str(e)}")
        logger.error(f"‚ùå Validation error: {e}")
    
    return validation


def create_leak_free_pipeline(
    model_name: str, 
    task_type: str, 
    column_types: Dict[str, List[str]],
    preprocessing_choices: Dict[str, Any],
    use_smote: bool = False,
    optimize_hyperparams: bool = False
) -> Tuple[Optional[Pipeline], Optional[Dict]]:
    """
    Cr√©e un pipeline sans data leakage en int√©grant le pr√©processeur DANS le pipeline.
    
    Args:
        model_name: Nom du mod√®le
        task_type: Type de t√¢che
        column_types: Types de colonnes d√©tect√©s
        preprocessing_choices: Options de pr√©traitement
        use_smote: Utiliser SMOTE (seulement si supervis√©)
        optimize_hyperparams: Optimiser les hyperparam√®tres
        
    Returns:
        Tuple (pipeline, param_grid)
    """
    try:
        model_config = get_model_config(task_type, model_name)
        if not model_config:
            logger.error(f"‚ùå Configuration non trouv√©e pour {model_name} ({task_type})")
            return None, None
            
        model = model_config["model"]
        param_grid = {}
        
        if optimize_hyperparams and "params" in model_config:
            param_grid = {f"model__{k}": v for k, v in model_config["params"].items()}
            logger.debug(f"Grille param√®tres pour {model_name}: {len(param_grid)} combinaisons")
        
        preprocessor = create_preprocessor(preprocessing_choices, column_types)
        if preprocessor is None:
            logger.error(f"‚ùå √âchec cr√©ation pr√©processeur pour {model_name}")
            return None, None
        
        validation_result = validate_preprocessor(preprocessor, pd.DataFrame({
            col: [0] for cols in column_types.values() for col in cols
        }))
        
        if not validation_result["is_valid"]:
            logger.warning(f"‚ö†Ô∏è Pr√©processeur avec issues: {validation_result['issues']}")
        
        pipeline_steps = [('preprocessor', preprocessor)]
        
        if use_smote and task_type == 'classification':
            smote_k = PREPROCESSING_CONSTANTS.get("SMOTE_K_NEIGHBORS", 5)
            random_state = TRAINING_CONSTANTS.get("RANDOM_STATE", 42)
            pipeline_steps.append(('smote', SMOTE(
                random_state=random_state,
                k_neighbors=smote_k
            )))
            logger.debug("SMOTE ajout√© au pipeline")
        
        pipeline_steps.append(('model', model))
        pipeline = Pipeline(pipeline_steps)
        
        logger.info(f"‚úÖ Pipeline leak-free cr√©√© pour {model_name} avec {len(pipeline_steps)} √©tapes")
        return pipeline, param_grid
        
    except Exception as e:
        logger.error(f"‚ùå Erreur cr√©ation pipeline pour {model_name}: {e}")
        return None, None


def train_single_model_supervised(
    model_name: str,
    pipeline: Pipeline,
    X_train: pd.DataFrame,
    y_train: pd.Series,
    X_test: pd.DataFrame,
    y_test: pd.Series,
    param_grid: Dict = None,
    task_type: str = 'classification',
    monitor: TrainingMonitor = None
) -> Dict[str, Any]:
    """Entra√Æne un mod√®le supervis√© avec validation crois√©e propre."""
    result = {
        "model_name": model_name,
        "success": False,
        "model": None,
        "training_time": 0,
        "error": None,
        "best_params": None,
        "cv_scores": None
    }
    
    start_time = time.time()
    
    try:
        if monitor:
            monitor.start_model(model_name)
        
        cv_folds = TRAINING_CONSTANTS.get("CV_FOLDS", 5)
        random_state = TRAINING_CONSTANTS.get("RANDOM_STATE", 42)
        n_jobs = TRAINING_CONSTANTS.get("N_JOBS", -1)
        
        if task_type == 'classification':
            cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)
            scoring = 'accuracy'
        else:
            cv = KFold(n_splits=cv_folds, shuffle=True, random_state=random_state)
            scoring = 'r2'
        
        if param_grid and len(param_grid) > 0:
            logger.info(f"üîç Optimisation hyperparam√®tres pour {model_name}")
            
            max_combinations = TRAINING_CONSTANTS.get("MAX_GRID_COMBINATIONS", 100)
            total_combinations = np.prod([len(v) for v in param_grid.values()])
            
            if total_combinations > max_combinations:
                logger.warning(f"Grille trop large ({total_combinations} combinaisons), limitation automatique")
                limited_param_grid = {}
                for k, v in param_grid.items():
                    limited_param_grid[k] = v[:2] if len(v) > 2 else v
                param_grid = limited_param_grid
            
            grid_search = GridSearchCV(
                pipeline, param_grid, cv=cv, scoring=scoring,
                n_jobs=n_jobs, verbose=0, error_score='raise'
            )
            
            grid_search.fit(X_train, y_train)
            
            result["model"] = grid_search.best_estimator_
            result["best_params"] = grid_search.best_params_
            result["cv_scores"] = {
                'mean': grid_search.best_score_,
                'std': grid_search.cv_results_['std_test_score'][grid_search.best_index_]
            }
            result["success"] = True
            
            logger.info(f"‚úÖ Optimisation termin√©e pour {model_name} - score: {grid_search.best_score_:.3f}")
            
        else:
            try:
                cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring=scoring)
                result["cv_scores"] = {'mean': cv_scores.mean(), 'std': cv_scores.std()}
                logger.info(f"üìä CV scores pour {model_name}: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
            except Exception as cv_error:
                logger.warning(f"‚ö†Ô∏è CV √©chou√©e pour {model_name}: {cv_error}")
                result["cv_scores"] = None
            
            pipeline.fit(X_train, y_train)
            result["model"] = pipeline
            result["success"] = True
        
        result["training_time"] = time.time() - start_time
        
        if monitor:
            resource_info = monitor.check_resources()
            logger.info(f"‚úÖ {model_name} entra√Æn√© en {result['training_time']:.2f}s - "
                       f"RAM: {resource_info.get('memory_percent', 0):.1f}%")
        
    except Exception as e:
        result["success"] = False
        result["error"] = str(e)
        result["training_time"] = time.time() - start_time
        logger.error(f"‚ùå Erreur entra√Ænement {model_name}: {e}")
    
    return result


def train_single_model_unsupervised(
    model_name: str,
    pipeline: Pipeline,
    X: pd.DataFrame,
    param_grid: Dict = None,
    monitor: TrainingMonitor = None
) -> Dict[str, Any]:
    """Entra√Æne un mod√®le non-supervis√© (clustering)."""
    result = {
        "model_name": model_name,
        "success": False,
        "model": None,
        "training_time": 0,
        "error": None,
        "best_params": None
    }
    
    start_time = time.time()
    
    try:
        if monitor:
            monitor.start_model(model_name)
        
        cv_folds = TRAINING_CONSTANTS.get("CV_FOLDS", 5)
        n_jobs = TRAINING_CONSTANTS.get("N_JOBS", -1)
        
        if param_grid and len(param_grid) > 0:
            logger.info(f"üîç Optimisation hyperparam√®tres clustering pour {model_name}")
            
            grid_search = GridSearchCV(
                pipeline, param_grid, cv=cv_folds,
                scoring='silhouette_score' if hasattr(pipeline, 'fit_predict') else 'silhouette',
                n_jobs=n_jobs, verbose=0
            )
            
            grid_search.fit(X)
            result["model"] = grid_search.best_estimator_
            result["best_params"] = grid_search.best_params_
            result["success"] = True
            
        else:
            pipeline.fit(X)
            result["model"] = pipeline
            result["success"] = True
        
        result["training_time"] = time.time() - start_time
        
        if monitor:
            resource_info = monitor.check_resources()
            logger.info(f"‚úÖ Clustering {model_name} termin√© en {result['training_time']:.2f}s")
        
    except Exception as e:
        result["success"] = False
        result["error"] = str(e)
        result["training_time"] = time.time() - start_time
        logger.error(f"‚ùå Erreur clustering {model_name}: {e}")
    
    return result


def evaluate_model_with_metrics_calculator(
    model, 
    X_test: pd.DataFrame, 
    y_test: pd.Series, 
    task_type: str,
    label_encoder: Any = None,
    X_data: pd.DataFrame = None
) -> Dict[str, Any]:
    """√âvalue un mod√®le en utilisant EvaluationMetrics."""
    try:
        metrics_calculator = EvaluationMetrics(task_type)
        
        if task_type == 'clustering':
            if X_data is None:
                return {"error": "Donn√©es X requises pour l'√©valuation non supervis√©e"}
            
            if hasattr(model, 'predict'):
                cluster_labels = model.predict(X_data)
            elif hasattr(model, 'labels_'):
                cluster_labels = model.labels_
            else:
                cluster_labels = model.fit_predict(X_data)
            
            metrics = metrics_calculator.calculate_unsupervised_metrics(X_data.values, cluster_labels)
            metrics['n_clusters'] = len(np.unique(cluster_labels[~np.isnan(cluster_labels)]))
            metrics['task_type'] = task_type
            metrics['n_samples'] = len(X_data)
            
        else:
            y_pred = model.predict(X_test)
            y_proba = None
            
            if hasattr(model, "predict_proba"):
                try:
                    y_proba = model.predict_proba(X_test)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Predict_proba non disponible: {e}")
            
            if task_type == 'classification':
                metrics = metrics_calculator.calculate_classification_metrics(
                    y_test.values, y_pred, y_proba
                )
            else:
                metrics = metrics_calculator.calculate_regression_metrics(
                    y_test.values, y_pred
                )
            
            metrics['task_type'] = task_type
            metrics['n_samples'] = len(X_test)
        
        if hasattr(metrics_calculator, 'error_messages') and metrics_calculator.error_messages:
            metrics['calculation_warnings'] = metrics_calculator.error_messages
        
        metrics['success'] = True
        return metrics
        
    except Exception as e:
        logger.error(f"‚ùå Erreur √©valuation avec EvaluationMetrics: {e}")
        return {
            "error": f"Erreur √©valuation: {str(e)}",
            "success": False,
            "task_type": task_type
        }


def train_models(
    df: pd.DataFrame,
    target_column: Optional[str],
    model_names: List[str],
    task_type: str,
    test_size: float = 0.2,
    optimize: bool = False,
    feature_list: List[str] = None,
    use_smote: bool = False,
    preprocessing_choices: Dict = None
) -> List[Dict[str, Any]]:
    """
    Orchestre l'entra√Ænement sans data leakage pour tous types de t√¢ches avec MLflow tracking.
    
    Args:
        df: DataFrame contenant les donn√©es
        target_column: Nom de la colonne cible (None pour clustering)
        model_names: Liste des noms de mod√®les √† entra√Æner
        task_type: Type de t√¢che ('classification', 'regression', 'clustering')
        test_size: Proportion du jeu de test (ignor√© pour clustering)
        optimize: Si True, effectue une recherche d'hyperparam√®tres
        feature_list: Liste des features √† utiliser (None = toutes sauf target)
        use_smote: Si True, applique SMOTE pour le d√©s√©quilibre de classes
        preprocessing_choices: Configuration du pr√©traitement
        
    Returns:
        Liste de dictionnaires contenant les r√©sultats pour chaque mod√®le
    """
    
    results = []
    monitor = TrainingMonitor()
    monitor.start_training()
    
    # Normalisation du type de t√¢che
    task_type = task_type.lower()
    if task_type == 'unsupervised':
        task_type = 'clustering'
        
    if task_type == 'clustering':
        target_column = None
        use_smote = False
        test_size = 0.0
    
    logger.info(f"D√©but entra√Ænement - Type: {task_type}, Mod√®les: {len(model_names)}, Target: {target_column}")
    
    # Configuration MLflow avec gestion s√©curis√©e
    mlflow_enabled = is_mlflow_available()
    if mlflow_enabled:
        try:
            experiment_name = f"{task_type}_experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            mlflow.set_experiment(experiment_name)
            logger.info(f"MLflow experiment initialized: {experiment_name}")
        except Exception as e:
            logger.warning(f"MLflow experiment setup failed: {e}")
            mlflow_enabled = False
    
    # D√©termination de la liste des caract√©ristiques
    if not feature_list:
        if target_column and target_column in df.columns:
            feature_list = [col for col in df.columns if col != target_column]
        else:
            feature_list = list(df.columns)
    
    # Limitation des caract√©ristiques
    max_features = TRAINING_CONSTANTS.get("MAX_FEATURES", 100)
    if len(feature_list) > max_features:
        logger.warning(f"Trop de features ({len(feature_list)}), limitation √† {max_features}")
        feature_list = feature_list[:max_features]
    
    # Configuration par d√©faut pour le pr√©traitement
    if preprocessing_choices is None:
        preprocessing_choices = {
            'numeric_imputation': PREPROCESSING_CONSTANTS.get("NUMERIC_IMPUTATION_DEFAULT", "mean"),
            'categorical_imputation': PREPROCESSING_CONSTANTS.get("CATEGORICAL_IMPUTATION_DEFAULT", "most_frequent"),
            'remove_constant_cols': True,
            'remove_identifier_cols': True,
            'scale_features': True,
            'scaling_method': PREPROCESSING_CONSTANTS.get("SCALING_METHOD", "standard"),
            'encoding_method': PREPROCESSING_CONSTANTS.get("ENCODING_METHOD", "onehot")
        }
    
    # Cr√©ation des r√©pertoires de sortie
    os.makedirs("models_output", exist_ok=True)
    os.makedirs("training_logs", exist_ok=True)

    # Pr√©paration des donn√©es
    logger.info("Pr√©paration des donn√©es...")
    X = df[feature_list].copy()
    
    if X.empty:
        logger.error("DataFrame d'entr√©e vide ou None")
        return [{"model_name": "Validation", "metrics": {"error": "DataFrame d'entr√©e vide ou None"}}]
    
    y = None
    label_encoder = None
    
    if task_type != 'clustering' and target_column:
        if target_column not in df.columns:
            logger.error(f"Colonne cible '{target_column}' non trouv√©e")
            return [{"model_name": "Validation", "metrics": {"error": f"Target '{target_column}' non trouv√©e"}}]
        
        y_raw = df[target_column].copy()
        y_encoded, label_encoder, _ = safe_label_encode(y_raw)
        y = pd.Series(y_encoded, index=y_raw.index, name=target_column)
    
    # Validation des donn√©es
    data_validation = validate_training_data(X, y, task_type)
    if not data_validation["is_valid"]:
        error_msg = f"Donn√©es invalides: {', '.join(data_validation['issues'])}"
        logger.error(error_msg)
        return [{"model_name": "Validation", "metrics": {"error": error_msg}}]
    
    for warning in data_validation["warnings"]:
        logger.warning(warning)
    
    # D√©tection des types de colonnes
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        column_types = auto_detect_column_types(X)
    
    # S√©paration des donn√©es pour les t√¢ches supervis√©es
    X_train, X_test, y_train, y_test = None, None, None, None
    
    if task_type != 'clustering':
        try:
            random_state = TRAINING_CONSTANTS.get("RANDOM_STATE", 42)
            stratification = y if task_type == 'classification' else None
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state, stratify=stratification
            )
            logger.info(f"Donn√©es splitt√©es: train={len(X_train)}, test={len(X_test)}")
        except Exception as split_error:
            logger.error(f"Erreur lors du split: {split_error}")
            return [{"model_name": "Split", "metrics": {"error": str(split_error)}}]
    
    # R√©cup√©ration des constantes
    successful_models = 0
    max_training_time = TRAINING_CONSTANTS.get("MAX_TRAINING_TIME", 3600)
    high_memory_threshold = TRAINING_CONSTANTS.get("HIGH_MEMORY_THRESHOLD", 85)
    max_sample_size = TRAINING_CONSTANTS.get("MAX_VISUALIZATION_SAMPLES", 1000)
    
    # Liste pour stocker les runs MLflow
    mlflow_runs = []
    
    for i, model_name in enumerate(model_names, 1):
        logger.info(f"Processing model {i}/{len(model_names)}: {model_name}")
        
        # V√©rification du temps d'entra√Ænement global
        total_duration = monitor.get_total_duration()
        if total_duration > max_training_time:
            logger.warning(f"Temps d'entra√Ænement d√©pass√© ({total_duration:.0f}s > {max_training_time}s)")
            results.append({
                "model_name": model_name,
                "metrics": {"error": "Temps d'entra√Ænement maximum d√©pass√©"}
            })
            continue
        
        # V√©rification des ressources syst√®me
        resource_info = monitor.check_resources()
        if resource_info.get('memory_percent', 0) > high_memory_threshold:
            logger.warning("M√©moire √©lev√©e, nettoyage...")
            gc.collect()
        
        # Cr√©ation du pipeline
        try:
            pipeline, param_grid = create_leak_free_pipeline(
                model_name=model_name, 
                task_type=task_type, 
                column_types=column_types,
                preprocessing_choices=preprocessing_choices, 
                use_smote=use_smote, 
                optimize_hyperparams=optimize
            )
        except Exception as pipeline_error:
            logger.error(f"Erreur cr√©ation pipeline pour {model_name}: {pipeline_error}")
            results.append({
                "model_name": model_name, 
                "metrics": {"error": f"Erreur cr√©ation du pipeline: {str(pipeline_error)}"}
            })
            continue
        
        if pipeline is None:
            results.append({"model_name": model_name, "metrics": {"error": "Pipeline None retourn√©"}})
            continue
        
        # Initialisation MLflow pour ce mod√®le
        run_id = None
        mlflow_active = mlflow_enabled
        
        if mlflow_active:
            try:
                timestamp = int(time.time())
                mlflow.start_run(run_name=f"{model_name}_{timestamp}")
                run_id = mlflow.active_run().info.run_id
                
                # Logs des param√®tres de configuration
                mlflow.log_param("task_type", task_type)
                mlflow.log_param("model_name", model_name)
                mlflow.log_param("n_samples", len(X))
                mlflow.log_param("n_features", len(feature_list))
                mlflow.log_param("use_smote", use_smote)
                mlflow.log_param("optimize_hyperparams", optimize)
                mlflow.log_param("test_size", test_size)
                
                for key, value in preprocessing_choices.items():
                    mlflow.log_param(f"preprocessing_{key}", str(value)[:250])
                
                logger.info(f"MLflow run started for {model_name}: {run_id}")
                
            except Exception as mlflow_error:
                logger.warning(f"MLflow run start failed for {model_name}: {mlflow_error}")
                mlflow_active = False
                run_id = None
        
        # Entra√Ænement du mod√®le
        try:
            if task_type == 'clustering':
                training_result = train_single_model_unsupervised(
                    model_name=model_name, 
                    pipeline=pipeline, 
                    X=X, 
                    param_grid=param_grid, 
                    monitor=monitor
                )
            else:
                training_result = train_single_model_supervised(
                    model_name=model_name, 
                    pipeline=pipeline, 
                    X_train=X_train, 
                    y_train=y_train,
                    X_test=X_test, 
                    y_test=y_test, 
                    param_grid=param_grid, 
                    task_type=task_type, 
                    monitor=monitor
                )
        except Exception as train_error:
            logger.error(f"Erreur entra√Ænement {model_name}: {train_error}")
            
            # Fermer le run MLflow en cas d'erreur
            if mlflow_active and run_id:
                try:
                    mlflow.log_param("status", "FAILED")
                    mlflow.log_param("error", str(train_error)[:250])
                    mlflow.end_run(status="FAILED")
                except:
                    pass
            
            results.append({
                "model_name": model_name,
                "metrics": {"error": f"Erreur entra√Ænement: {str(train_error)}"},
                "training_time": 0
            })
            gc.collect()
            continue
        
        # Traitement des r√©sultats d'entra√Ænement
        if training_result["success"] and training_result["model"] is not None:
            try:
                # √âvaluation du mod√®le
                if task_type == 'clustering':
                    metrics = evaluate_model_with_metrics_calculator(
                        model=training_result["model"], 
                        X_test=None, 
                        y_test=None,
                        task_type=task_type, 
                        X_data=X
                    )
                else:
                    metrics = evaluate_model_with_metrics_calculator(
                        model=training_result["model"], 
                        X_test=X_test, 
                        y_test=y_test,
                        task_type=task_type, 
                        label_encoder=label_encoder
                    )
                
                # Sauvegarde du mod√®le
                timestamp = int(time.time())
                model_filename = f"{model_name.replace(' ', '_').lower()}_{task_type}_{timestamp}.joblib"
                model_path = os.path.join("models_output", model_filename)
                
                joblib.dump(training_result["model"], model_path)
                logger.info(f"Mod√®le sauvegard√©: {model_path}")
                
                # Pr√©paration des √©chantillons pour visualisations
                if task_type == 'clustering':
                    sample_size = min(max_sample_size, len(X))
                    X_sample = X[:sample_size].copy()
                    labels_sample = training_result["model"].predict(X_sample)
                    X_test_vis = None
                    y_test_vis = None
                    X_train_vis = None
                    y_train_vis = None
                else:
                    sample_size_test = min(max_sample_size, len(X_test))
                    sample_size_train = min(max_sample_size, len(X_train))
                    
                    X_test_vis = X_test.iloc[:sample_size_test].copy() if hasattr(X_test, 'iloc') else X_test[:sample_size_test].copy()
                    y_test_vis = y_test.iloc[:sample_size_test].copy() if hasattr(y_test, 'iloc') else y_test[:sample_size_test].copy()
                    X_train_vis = X_train.iloc[:sample_size_train].copy() if hasattr(X_train, 'iloc') else X_train[:sample_size_train].copy()
                    y_train_vis = y_train.iloc[:sample_size_train].copy() if hasattr(y_train, 'iloc') else y_train[:sample_size_train].copy()
                    X_sample = X_test_vis.copy()
                    labels_sample = None
                
                # Construction du r√©sultat
                result = {
                    "model_name": model_name,
                    "metrics": metrics,
                    "model_path": model_path,
                    "training_time": training_result["training_time"],
                    "best_params": training_result.get("best_params"),
                    "cv_scores": training_result.get("cv_scores"),
                    "model": training_result["model"],
                    "label_encoder": label_encoder,
                    "feature_names": feature_list,
                    "task_type": task_type,
                    "timestamp": timestamp,
                    "X_test": X_test_vis,
                    "y_test": y_test_vis,
                    "X_train": X_train_vis,
                    "y_train": y_train_vis,
                    "X_sample": X_sample,
                    "labels": labels_sample,
                }
                
                # Logging MLflow
                if mlflow_active and run_id:
                    try:
                        # Log des m√©triques
                        for metric_name, metric_value in metrics.items():
                            if isinstance(metric_value, (int, float)) and not np.isnan(metric_value):
                                mlflow.log_metric(metric_name, float(metric_value))
                        
                        # Log des meilleurs hyperparam√®tres
                        if training_result.get("best_params"):
                            for param_name, param_value in training_result["best_params"].items():
                                mlflow.log_param(f"best_{param_name}", str(param_value)[:250])
                        
                        # Log des scores de validation crois√©e
                        if training_result.get("cv_scores") is not None:
                            cv_scores = training_result["cv_scores"]
                            if isinstance(cv_scores, (list, np.ndarray)) and len(cv_scores) > 0:
                                mlflow.log_metric("cv_mean_score", float(np.mean(cv_scores)))
                                mlflow.log_metric("cv_std_score", float(np.std(cv_scores)))
                        
                        # Log du mod√®le sklearn
                        mlflow.sklearn.log_model(
                            training_result["model"], 
                            artifact_path="model",
                            registered_model_name=f"{task_type}_{model_name.replace(' ', '_')}_{timestamp}"
                        )
                        
                        # Log des artefacts
                        mlflow.log_artifact(model_path)
                        mlflow.log_metric("training_time", float(training_result["training_time"]))
                        mlflow.log_metric("memory_percent", float(resource_info.get('memory_percent', 0)))
                        mlflow.log_metric("cpu_percent", float(resource_info.get('cpu_percent', 0)))
                        
                        # Log de la validation des donn√©es
                        mlflow.log_dict(data_validation, "data_validation.json")
                        
                        # R√©cup√©rer les informations du run pour l'interface
                        run_info = mlflow.active_run()
                        mlflow_run_data = {
                            'run_id': run_info.info.run_id,
                            'status': 'FINISHED',
                            'start_time': run_info.info.start_time,
                            'end_time': int(time.time() * 1000),
                            'tags.mlflow.runName': f"{model_name}_{timestamp}",
                        }
                        
                        # Ajouter les m√©triques avec pr√©fixe
                        for k, v in metrics.items():
                            if isinstance(v, (int, float)) and not np.isnan(v):
                                mlflow_run_data[f'metrics.{k}'] = float(v)
                        
                        # Ajouter les param√®tres avec pr√©fixe
                        for k, v in preprocessing_choices.items():
                            mlflow_run_data[f'params.preprocessing_{k}'] = str(v)[:100]
                        
                        mlflow_runs.append(mlflow_run_data)
                        
                        logger.info(f"MLflow logged successfully for {model_name} - Run ID: {run_id}")
                        
                    except Exception as mlflow_log_error:
                        logger.warning(f"MLflow logging failed for {model_name}: {mlflow_log_error}")
                    
                    finally:
                        # Toujours fermer le run
                        try:
                            mlflow.end_run()
                        except:
                            pass
                
                results.append(result)
                successful_models += 1
                
                logger.info(f"{model_name} - succ√®s en {training_result['training_time']:.2f}s")
                
            except Exception as eval_error:
                logger.error(f"Erreur √©valuation {model_name}: {eval_error}")
                
                # Fermer le run MLflow en cas d'erreur d'√©valuation
                if mlflow_active and run_id:
                    try:
                        mlflow.log_param("status", "FAILED")
                        mlflow.log_param("error", f"Evaluation error: {str(eval_error)[:250]}")
                        mlflow.end_run(status="FAILED")
                    except:
                        pass
                
                results.append({
                    "model_name": model_name,
                    "metrics": {"error": f"Erreur √©valuation: {str(eval_error)}"},
                    "training_time": training_result.get("training_time", 0)
                })
        
        else:
            # √âchec de l'entra√Ænement
            error_msg = training_result.get("error", "Erreur inconnue lors de l'entra√Ænement")
            logger.error(f"√âchec entra√Ænement {model_name}: {error_msg}")
            
            # Fermer le run MLflow en cas d'√©chec
            if mlflow_active and run_id:
                try:
                    mlflow.log_param("status", "FAILED")
                    mlflow.log_param("error", str(error_msg)[:250])
                    mlflow.end_run(status="FAILED")
                except:
                    pass
            
            results.append({
                "model_name": model_name,
                "metrics": {"error": error_msg},
                "training_time": training_result.get("training_time", 0)
            })
        
        # Nettoyage m√©moire apr√®s chaque mod√®le
        gc.collect()
    
    # G√©n√©ration du rapport final
    total_time = monitor.get_total_duration()
    monitor_summary = monitor.get_summary()
    
    training_log = {
        "timestamp": datetime.now().isoformat(),
        "task_type": task_type,
        "target_column": target_column,
        "models_attempted": len(model_names),
        "models_successful": successful_models,
        "total_training_time": total_time,
        "monitor_summary": monitor_summary,
        "data_validation": data_validation,
        "results_summary": [
            {
                "model_name": r["model_name"],
                "success": "metrics" in r and "error" not in r.get("metrics", {}),
                "training_time": r.get("training_time", 0)
            } for r in results
        ]
    }
    
    # Sauvegarde du log d'entra√Ænement
    log_filename = f"training_log_{int(time.time())}.json"
    log_path = os.path.join("training_logs", log_filename)
    
    try:
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(training_log, f, indent=2, ensure_ascii=False)
        logger.info(f"Log d'entra√Ænement sauvegard√©: {log_path}")
    except Exception as log_error:
        logger.warning(f"√âchec sauvegarde log: {log_error}")
    
    # Log MLflow final
    if mlflow_enabled:
        try:
            # Cr√©er un run global pour les statistiques
            with mlflow.start_run(run_name=f"training_summary_{int(time.time())}"):
                mlflow.log_artifact(log_path)
                mlflow.log_metric("total_training_time", float(total_time))
                mlflow.log_metric("models_successful", int(successful_models))
                mlflow.log_metric("models_attempted", int(len(model_names)))
                mlflow.log_metric("success_rate", float(successful_models / len(model_names) * 100 if model_names else 0))
                mlflow.log_dict(monitor_summary, "monitor_summary.json")
                
            logger.info("MLflow summary logged")
        except Exception as mlflow_final_error:
            logger.warning(f"MLflow final logging failed: {mlflow_final_error}")
    
    # Stocker les runs MLflow dans la session state (pour Streamlit)
    if mlflow_runs:
        try:
            import streamlit as st
            if 'mlflow_runs' not in st.session_state:
                st.session_state.mlflow_runs = []
            st.session_state.mlflow_runs.extend(mlflow_runs)
            logger.info(f"{len(mlflow_runs)} runs MLflow stock√©s pour l'interface")
        except ImportError:
            # Streamlit n'est pas disponible (mode CLI/API)
            pass
        except Exception as session_error:
            logger.warning(f"√âchec stockage runs MLflow en session: {session_error}")
    
    logger.info(f"Entra√Ænement termin√©: {successful_models}/{len(model_names)} mod√®les r√©ussis en {total_time:.2f}s")
    
    # Nettoyage final
    gc.collect()
    
    return results

def cleanup_models_directory(max_files: int = None):
    """Nettoie le dossier des mod√®les pour √©viter l'accumulation."""
    if max_files is None:
        max_files = TRAINING_CONSTANTS.get("MAX_MODEL_FILES", 50)
    
    try:
        if not os.path.exists("models_output"):
            return
            
        model_files = []
        for filename in os.listdir("models_output"):
            if filename.endswith('.joblib'):
                filepath = os.path.join("models_output", filename)
                model_files.append((filepath, os.path.getctime(filepath)))
        
        model_files.sort(key=lambda x: x[1])
        
        if len(model_files) > max_files:
            for i in range(len(model_files) - max_files):
                filepath, _ = model_files[i]
                os.remove(filepath)
                logger.info(f"üóëÔ∏è Fichier mod√®le supprim√©: {filepath}")
                
    except Exception as e:
        logger.error(f"‚ùå Erreur nettoyage dossier mod√®les: {e}")


# Nettoyage automatique au chargement du module
cleanup_models_directory()

# Export des fonctions principales
__all__ = [
    'TrainingMonitor',
    'validate_training_data', 
    'create_leak_free_pipeline',
    'train_single_model_supervised',
    'train_single_model_unsupervised',
    'train_models',
    'cleanup_models_directory',
    'is_mlflow_available',
    'MLFLOW_AVAILABLE'
]